{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Amazon Forecast\n",
    "\n",
    "Although you can use Amazon Forecast by interacting with the API directly, we created this notebook to help users get started quickly.  We picked a typical timeseries forecasting scenario so stepping through this notebook will show you how to setup access, import your data, create predictors, and finally, use your predictors.\n",
    "\n",
    "### Prerequisites\n",
    "You will need the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/installing.html) to setup access to the service and interact with the API.  For more informations about APIs, please check the [documentation](https://docs.aws.amazon.com/forecast/latest/dg/what-is-forecast.html).\n",
    "\n",
    "### Table of contents\n",
    "* [Setup](#setup)\n",
    "* [Test Setup](#hello)\n",
    "* [Forecasting with Amazon Forecast](#forecastingExample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Preview Access<a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Forecast is not generally available yet.  To use the service during the preview, we need to configure the CLI (once for every CLI instance) so it knows about the service and set up IAM Policies in the AWS Account (once for every AWS Account) so Forecast can use data and deploy predictors in your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure CLI\n",
    "Remember that you need to do this once for each CLI instance you would like to use to interact with ForecastQuery and Forecast, the two APIs provided by Amazon Forecast.\n",
    "\n",
    "First, confirm that your AWS CLI is configured to interact with your AWS Account as expected using:\n",
    "```\n",
    "aws ec2 describe-regions\n",
    "```\n",
    "\n",
    "Next use the following commands to \"teach\" the CLI about the two new APIs (remember to specify the correct location for the files you obtained from the GitHub repo where you got this notebook).\n",
    "```\n",
    "aws configure add-model --service-model file://../sdk/forecastquery-2018-06-26.normal.json --service-name forecastquery\n",
    "aws configure add-model --service-model file://../sdk/forecast-2018-06-26.normal.json --service-name forecast\n",
    "\n",
    "```\n",
    "\n",
    "To confirm that this worked as expected, try getting help for the `list-recipes` command.\n",
    "```\n",
    "aws forecast list-recipes help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure AWS Account (IAM setup)\n",
    "During the preview period, you'll need to do this once for every AWS Account where you intend to use Amazon Forecast.\n",
    "\n",
    "First, please create an Amazon S3 bucket in `us-west-2` that you intend to use for the rest of this exercise.\n",
    "Remember that you'll need to find a unique name for your bucket.\n",
    "```\n",
    "aws s3 s3://my-amazon-forecast-labs --region us-west-2\n",
    "```\n",
    "*And yes, you can reuse an existing bucket, but you will need to tweak the IAM policies to use it.*\n",
    "\n",
    "\n",
    "Finally, use `setup_forecast_permissions` utility (also available at the GitHub repo where you got this notebook) to setup the relevant IAM policies.\n",
    "```\n",
    "# remember to use your bucket name here\n",
    "python3 ./setup_forecast_permissions.py my-amazon-forecast-labs\n",
    "```\n",
    "\n",
    "When this is done, you should see that the relevant IAM user or role will have a `ForecastUserPolicy` and a `PassRoleToForecastPolicy` attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup <a class=\"anchor\" id=\"hello\"></a>\n",
    "Let's say \"Hi\" to Amazon Forecast and use the simple ListRecipes API method. This method returns a list of the global recipes that you can use as a part of your forecasting solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites : 1 time install only\n",
    "# !pip install boto3\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "\n",
    "# remember to use your bucket name here\n",
    "bucketName='my-amazon-forecast-labs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name='us-west-2') #us-east-1 is also supported\n",
    "\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecastquery = session.client(service_name='forecastquery')\n",
    "\n",
    "accountId = boto3.client('sts').get_caller_identity().get('Account')\n",
    "# this role should have been created in the IAM setup step above\n",
    "roleArn = 'arn:aws:iam::%s:role/amazonforecast'%accountId\n",
    "print('IAM role for Amazon Forecast:  %s'%roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipeList = forecast.list_recipes()\n",
    "recipeList['RecipeNames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If this ran successfully, kudos! If there are any errors at this point runing the following list_recipes, please contact us at the [AWS support forum](https://forums.aws.amazon.com/forum.jspa?forumID=327)\n",
    "\n",
    "Check out the documentation for more information about these recipes.  For this notebook, we'll be using the [Multi-Quantile Recurrent Neural Network (MQRNN)](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-mqrnn.html) recipe to forecast demand with a one-dimensional time series dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with Amazon Forecast<a class=\"anchor\" id=\"forecastingExample\"></a>\n",
    "### Preparing your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Amazon Forecast, a dataset is a collection of file(s) which contain data that are relevant for a forecasting task. A dataset must conform to the schema provided by Amazon Forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we use the individual household electric power consumption dataset. (Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.) \n",
    "\n",
    "We aggregate the usage data hourly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon forecast can import data from Amazon S3. We first explore the data locally to see the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, do this once to get the dataset\n",
    "# !wget \"https://raw.githubusercontent.com/aws-samples/amazon-forecast-samples/master/data/item-demand-time.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"item-demand-time.csv\", dtype = object)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client('s3')\n",
    "accountId = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketName = 'amazon-forecast-%s-data'%accountId \n",
    "# remember that we've already set bucketName above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the dataset to S3 (set file location to where you have this file saved)\n",
    "s3.upload_file(Filename=\"../data/item-demand-time.csv\", Bucket=bucketName, Key=\"elec_data/item-demand-time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CreateDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on `Domain` and dataset types, check out the [documentation](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-domains-ds-types.html). For this example, we are using the [CUSTOM](https://docs.aws.amazon.com/forecast/latest/dg/custom-domain.html) domain with 3 required attributes `timestamp`, `target_value` and `item_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FREQUENCY = \"H\" \n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd hh:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'workshop'\n",
    "datasetName = project + '_ds'\n",
    "datasetGroupName = project + '_gp'\n",
    "s3DataPath = \"s3://\" + bucketName + \"/private/labs/forecast/elec_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"target_value\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "response = forecast.create_dataset(\n",
    "                    Domain=\"CUSTOM\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DataFormat='CSV',\n",
    "                    DatasetName=datasetName,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    TimeStampFormat=TIMESTAMP_FORMAT,\n",
    "                    Schema = schema\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did it work?\n",
    "forecast.describe_dataset(DatasetName=datasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add the new Dataset to a Dataset Group so we can use it to train a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.create_dataset_group(DatasetGroupName=datasetGroupName, RoleArn=roleArn, DatasetNames=[datasetName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dataset Group, you can also update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did it work?\n",
    "forecast.describe_dataset_group(DatasetGroupName=datasetGroupName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Import Job\n",
    "Next, let's create an import job to make the data available to Amazon Forecast for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_import_job_response = forecast.create_dataset_import_job(\n",
    "    DatasetName=datasetName, Delimiter=',', DatasetGroupName =datasetGroupName, S3Uri= s3DataPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_versionId=ds_import_job_response['VersionId']\n",
    "print(ds_versionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of dataset; we can continue once the status changes from **CREATING** to **ACTIVE**. Depending on the data size, this can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetName=datasetName,VersionId=ds_versionId)['Status']\n",
    "    print('.', end='')\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'FAILED':\n",
    "        sleep(10)\n",
    "    else:\n",
    "        break\n",
    "print('')\n",
    "print(dataImportStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did it work?\n",
    "forecast.describe_dataset_import_job(DatasetName=datasetName, VersionId=ds_versionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your recipe\n",
    "We will use the MQRNN recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipesResponse = forecast.list_recipes()\n",
    "recipesResponse['RecipeNames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more information about a recipe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_recipe(RecipeName='forecast_MQRNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a solution with customer forecast horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MQRNN recipe allows us to choose how many intervals (in future) to predict using the forecast horizon.  For weekly data, a value of 1 implies one week.  Our example uses hourly data and we want to forecast the next day, so we can set the forecast horizon to `24`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorName = project + '_mqrnn'\n",
    "forecastHorizon = 24\n",
    "createPredictorResponse = forecast.create_predictor(\n",
    "    RecipeName='forecast_MQRNN',\n",
    "    DatasetGroupName= datasetGroupName,\n",
    "    PredictorName=predictorName, \n",
    "    ForecastHorizon = forecastHorizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorVerionId = createPredictorResponse['VersionId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.list_predictor_versions(PredictorName=predictorName)['PredictorVersions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the predictor; when the status changes from **CREATING** to **ACTIVE**, we can continue to the next step.  Depending on data size, model selection, and hyper parametersï¼Œit can take 10 mins to more than one hour to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    predictorStatus = forecast.describe_predictor(PredictorName=predictorName,VersionId=predictorVerionId)['Status']\n",
    "    # print(predictorStatus)\n",
    "    print('.', end='')\n",
    "    if predictorStatus != 'ACTIVE' and predictorStatus != 'FAILED':\n",
    "        sleep(10)\n",
    "    else:\n",
    "        break\n",
    "print('')\n",
    "print(predictorStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Error Metrics\n",
    "Let's take a look at how well we can expect our predictor to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics show the error loss data for each quantile\n",
    "forecastquery.get_accuracy_metrics(PredictorName=predictorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.deploy_predictor(PredictorName=predictorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take anywhere from 10 minutes to an hour; the predictor will be ready for use when the status becomes **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployedPredictorsResponse=forecast.list_deployed_predictors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    deployedPredictorStatus = forecast.describe_deployed_predictor(PredictorName=predictorName)['Status']\n",
    "    print('.', end='')\n",
    "    if deployedPredictorStatus != 'ACTIVE' and deployedPredictorStatus != 'FAILED':\n",
    "        sleep(10)\n",
    "    else:\n",
    "        break\n",
    "print('')\n",
    "print(deployedPredictorStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the solution is deployed and forecast results are ready, you can view them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastResponse = forecastquery.get_forecast(\n",
    "    PredictorName=predictorName,\n",
    "    Interval=\"hour\",\n",
    "    Filters={\"item_id\":\"client_12\"}\n",
    ")\n",
    "\n",
    "df = pd.merge(\n",
    "    left=pd.DataFrame(forecastResponse['Forecast']['Predictions']['mean']).rename(columns={ 'Val': 'p10'}),\n",
    "    right=pd.DataFrame(forecastResponse['Forecast']['Predictions']['p10']).rename(columns={ 'Val': 'p50'})\n",
    ")\n",
    "df = pd.merge(\n",
    "    left=df,\n",
    "    right=pd.DataFrame(forecastResponse['Forecast']['Predictions']['p90']).rename(columns={ 'Val': 'p90'})\n",
    ")\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot out the forecast..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(title=\"Forecast\", sharey=True, figsize=(18, 9), xticks=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can batch export forecast to s3 bucket. To do so an role with s3 put access is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.create_forecast_export_job(ForecastId=forecastId, OutputPath={\"S3Uri\": s3DataPath,\"RoleArn\":roleArn})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
