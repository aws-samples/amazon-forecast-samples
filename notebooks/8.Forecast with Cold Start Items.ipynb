{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast with Cold Start Items\n",
    "\n",
    "Consider the situation where a set of related new items got introduced into the catalog, one needs to forecast the future values for those items. In such a situation, little to none demand history is available for those items. This scenario is coined as \"cold-start problem.\" \n",
    "\n",
    "<img src=\"BlogImages/amazon_forecast.png\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this notebook, we walk through the process of generating forecasts for cold start items. Notice that <b>only DeepAR+ supports cold start.</b> At a high level, the flow can be summarized as follows. \n",
    "\n",
    "1. We follow the complete process with non-cold start items as in the previous notebooks such as \n",
    " * [4. Getting_started_with_DeepAR+.ipynb](https://github.com/aws-samples/amazon-forecast-samples/blob/master/notebooks/4.%20Getting_started_with_DeepAR%2B.ipynb) or \n",
    " * [6. Incorporating_Related_Time_Series_dataset_to_your_Predictor.ipynb](https://github.com/aws-samples/amazon-forecast-samples/blob/master/notebooks/6.Incorporating_Related_Time_Series_dataset_to_your_Predictor.ipynb). \n",
    " \n",
    " The major addition is that, *the item meta information for both the cold start and non-cold start items are imported in the system*. \n",
    "\n",
    "2. Create another dataset with the cold start items, and create forecasts for those items. Here for simplicity, we only use target time series only dataset, but related time series can be incorporated as well. \n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "* Step 0: [Setting up](#setup)\n",
    "* Step 1: [Preparing the Datasets](#prepare)\n",
    "* Step 2: [Importing the Data for Non-Cold Start Items](#import)\n",
    " * Step 2a: [Creating a Dataset Group](#create)\n",
    " * Step 2b: [Creating a Target Dataset](#target)\n",
    " * Step 2c: [Creating a Item Meta Information Dataset](#related)\n",
    " * Step 2d: [Update the Dataset Group](#update)\n",
    " * Step 2e: [Creating a Target Time Series Dataset Import Job](#targetImport)\n",
    " * Step 2f: [Creating a Item Meta Information Dataset Import Job](#relatedImport)\n",
    "* Step 3: [Create the DeepAR+ Predictor](#algo)\n",
    "* Step 4: [Create a Forecast for non-Cold Start Items](#forecast)\n",
    "\n",
    "The above steps complete the model training with the non-Cold Start items. Now we ready to import the cold start items, and generate their forecats. \n",
    "\n",
    "* Step 5: [Create a Cold-Start Target Time Series Dataset Import Job](#coldStartImport)\n",
    "* Step 6: [Create a Forecast for the cold start items](#createColdStart) \n",
    "* Step 7: [Querying the Forecasts](#query)\n",
    "* Step 8: [Exporting the Forecasts](#export)\n",
    "* Step 9: [Clearning up your Resources](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0:  First let us setup Amazon Forecast<a class=\"anchor\" id=\"setup\">\n",
    "\n",
    "This section sets up the permissions and relevant endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the S3 bucket name and region name for this lesson.\n",
    "\n",
    "- If you don't have an S3 bucket, create it first on S3.\n",
    "- Although we have set the region to us-west-2 as a default value below, you can choose any of the regions that the service is available in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget_bucket = util.create_text_widget( \"bucketName\", \"input your S3 bucket name\" )\n",
    "text_widget_region = util.create_text_widget( \"region\", \"input region name.\", default_value=\"us-west-2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketName = text_widget_bucket.value\n",
    "assert bucketName, \"bucketName not set.\"\n",
    "\n",
    "region = text_widget_region.value\n",
    "assert region, \"region not set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region) \n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role to provide to Amazon Forecast.\n",
    "role_arn = util.get_or_create_role_arn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preparing the Datasets<a class=\"anchor\" id=\"prepare\">\n",
    "    \n",
    "    \n",
    "Here we use a synthetic dataset based on [electricity]() dataset, which consists of the hourly time series for 370 households (with item id 0 to 369). \n",
    "\n",
    "In this hypothetical senario, our goal is to generate forecasts for 4 new customers with item id 370 to 373. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipLocalFilePath = \"../data/cold-start/test.csv.gz\"\n",
    "localFilePath = \"../data/cold-start/test.csv\"\n",
    "\n",
    "util.extract_gz( zipLocalFilePath, localFilePath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(zipLocalFilePath, dtype = object)\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['target_value'] = tdf['target_value'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot one time series first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf['item_id'] == 'client_1'][-24*7*2:]\\\n",
    "    .plot(x='timestamp', y='target_value', figsize=(15, 8)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use an item meta information dataset that contains the information for both the non-cold start items (client 0 to 369) and cold start items (client 370 to 373). We call this meta information \"type\" in this specific case. Only one categorical feature is used in this demo, but in practice one normally has multiple categorical features. \n",
    "\n",
    "Note that for cold start items where little to none demand history exists, the algorithm can only \"transfer\" information from the existing items to the new ones through the meta information. Therefore, having informative and high quality meta data is the key for a good cold-start forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this metadata contains the cold start items' metadata as well.\n",
    "localItemMetaDataFilePath = \"../data/cold-start/itemMetaData.csv\"\n",
    "imdf = pd.read_csv(localItemMetaDataFilePath, dtype = object)\n",
    "\n",
    "imdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following figure shows the histogram of the category \"type.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdf['type'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountId = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTimeseriesDatakey = \"cold-start/test.csv\"\n",
    "\n",
    "s3.upload_file(Filename=localFilePath, Bucket = bucketName, Key = f\"{targetTimeseriesDatakey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemMetaDatakey = \"cold-start/itemMetaData.csv\"\n",
    "\n",
    "s3.upload_file(Filename=localItemMetaDataFilePath, Bucket = bucketName, Key = f\"{itemMetaDatakey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"coldstart_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we specify key input data and forecast parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"H\"\n",
    "forecast_horizon = 48\n",
    "timestamp_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "delimiter = ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a. Creating a Dataset Group<a class=\"anchor\" id=\"create\">\n",
    "First let's create a dataset group and then update it later to add our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = f\"{project}_grp\"\n",
    "dataset_arns = []\n",
    "create_dataset_group_response = forecast.create_dataset_group(Domain=\"CUSTOM\",\n",
    "                                                          DatasetGroupName=dataset_group,\n",
    "                                                          DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating dataset group {dataset_group}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b. Creating a Target Dataset<a class=\"anchor\" id=\"target\">\n",
    "In this example, we will define a target time series. This is a required dataset to use the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we specify the target time series name af_demo_ts_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_name = f\"{project}_ts\"\n",
    "print(ts_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the schema of our dataset below. Make sure the order of the attributes (columns) matches the raw \n",
    "data in the files. We follow the same three attribute format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_schema_val = [{\"AttributeName\": \"timestamp\", \"AttributeType\": \"timestamp\"},\n",
    "                {\"AttributeName\": \"target_value\", \"AttributeType\": \"float\"},\n",
    "                {\"AttributeName\": \"item_id\", \"AttributeType\": \"string\"}]\n",
    "ts_schema = {\"Attributes\": ts_schema_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating target dataset {ts_dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"CUSTOM\",\n",
    "                               DatasetType='TARGET_TIME_SERIES',\n",
    "                               DatasetName=ts_dataset_name,\n",
    "                               DataFrequency=freq,\n",
    "                               Schema=ts_schema\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=ts_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c. Creating ItemMetaData Dataset<a class=\"anchor\" id=\"related\">\n",
    "In this example, we will define a Item Metadata Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the related time series name af_demo_rts_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata_dataset_name = f\"{project}_meta\"\n",
    "print(item_metadata_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the schema of your dataset here. Make sure the order of columns matches the raw data files. We follow the same three column format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_schema_val = [{\"AttributeName\": \"item_id\", \"AttributeType\": \"string\"},\n",
    "              {\"AttributeName\": \"category\", \"AttributeType\": \"string\"}]\n",
    "meta_schema = {\"Attributes\": meta_schema_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating related dataset {meta_schema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"CUSTOM\",\n",
    "                               DatasetType='ITEM_METADATA',\n",
    "                               DatasetName=item_metadata_dataset_name,\n",
    "                               Schema=meta_schema\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=meta_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d. Updating the dataset group with the datasets we created<a class=\"anchor\" id=\"update\">\n",
    "You can have multiple datasets under the same dataset group. Update it with the datasets we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(ts_dataset_arn)\n",
    "dataset_arns.append(meta_dataset_arn)\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2e. Creating a Target Time Series Dataset Import Job<a class=\"anchor\" id=\"targetImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_response = forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                                             DatasetArn=ts_dataset_arn,\n",
    "                                                             DataSource= {\n",
    "                                                                 \"S3Config\" : {\n",
    "                                                                     \"Path\": f\"s3://{bucketName}/{targetTimeseriesDatakey}\",\n",
    "                                                                     \"RoleArn\": role_arn\n",
    "                                                                 } \n",
    "                                                             },\n",
    "                                                             TimestampFormat=timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_arn=ts_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2f. Creating a Item Meta Data Dataset Import Job<a class=\"anchor\" id=\"relatedImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dataset_import_job_response = forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                                             DatasetArn=meta_dataset_arn,\n",
    "                                                             DataSource= {\n",
    "                                                                 \"S3Config\" : {\n",
    "                                                                     \"Path\": f\"s3://{bucketName}/{itemMetaDatakey}\",\n",
    "                                                                     \"RoleArn\": role_arn\n",
    "                                                                 } \n",
    "                                                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dataset_import_job_arn=meta_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=meta_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Create  Predictor with the datasets<a class=\"anchor\" id=\"algo\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'Deep_AR_Plus'\n",
    "algorithm_arn_deep_ar_plus = algorithm_arn + algorithm\n",
    "predictor_name_deep_ar = f'{project}_{algorithm.lower()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[{predictor_name_deep_ar}] Creating predictor {predictor_name_deep_ar} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_predictor_response = forecast.create_predictor(PredictorName=predictor_name_deep_ar,\n",
    "                                                  AlgorithmArn=algorithm_arn_deep_ar_plus,\n",
    "                                                  ForecastHorizon=forecast_horizon,\n",
    "                                                  PerformAutoML=False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": dataset_group_arn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": freq}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_arn_deep_ar = create_predictor_response['PredictorArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Creating a Forecast<a class=\"anchor\" id=\"forecast\">\n",
    "\n",
    "Next we re-train with the full dataset, and create the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Done fetching accuracy numbers. Creating forecaster for DeepAR+ ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name_deep_ar = f'{project}_deep_ar_plus_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response_deep_ar = forecast.create_forecast(ForecastName=forecast_name_deep_ar,\n",
    "                                                        PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar = create_forecast_response_deep_ar['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Creating a Cold-Start Target Time Series Dataset Import Job<a class=\"anchor\" id=\"coldStartImport\">\n",
    "    \n",
    "Now we are ready to generate the forecasts for the cold start problem. Notice that there is a system constraint such that at least 5 rows needs to be present for each item. Therefore, for the item that has less than 5 observations, we fill in with NaNs. In the following example, both Client 370 and 372 have zero observation, i.e., pure cold-start problem, while the other two have 5 target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localColdStartDataFilePath = \"../data/cold-start/coldStartTargetData.csv\"\n",
    "# validation logic for at least 5 items\n",
    "cstdf = pd.read_csv(localColdStartDataFilePath, dtype = object)\n",
    "cstdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldStartTargetTimeseriesDatakey = \"cold-start/coldStartTargetData.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(Filename=localColdStartDataFilePath, \n",
    "               Bucket = bucketName, \n",
    "               Key = f\"{coldStartTargetTimeseriesDatakey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cold_start_dataset_import_job_response = forecast.create_dataset_import_job(DatasetImportJobName=dataset_group+\"_2\",\n",
    "                                                             DatasetArn=ts_dataset_arn,\n",
    "                                                             DataSource= {\n",
    "                                                                 \"S3Config\" : {\n",
    "                                                                     \"Path\": f\"s3://{bucketName}/{coldStartTargetTimeseriesDatakey}\",\n",
    "                                                                     \"RoleArn\": role_arn\n",
    "                                                                 } \n",
    "                                                             },\n",
    "                                                             TimestampFormat=timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cold_start_dataset_import_job_arn = ts_cold_start_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_import_job(DatasetImportJobArn=ts_cold_start_dataset_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_cold_start_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Create a ColdStart Item Forecast<a class=\"anchor\" id=\"createColdStart\">\n",
    "    \n",
    "    \n",
    "Now we are ready to create the forecasts for all cold start items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name_deep_ar_2 = f'{project}_deep_ar_plus_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response_deep_ar_2 = forecast.create_forecast(ForecastName=forecast_name_deep_ar_2,\n",
    "                                                        PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar_2 = create_forecast_response_deep_ar_2['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar_2))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Querying the ColdStart Item Forecast<a class=\"anchor\" id=\"query\">\n",
    "    \n",
    "Now we plot the forecast, where the first vertical line is the forecast start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_response_deep_2 = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar_2,\n",
    "    Filters={\"item_id\": \"client_370\"})\n",
    "\n",
    "\n",
    "fcst = forecast_response_deep_2['Forecast']['Predictions']\n",
    "time_stamp = list(map(lambda x: pd.to_datetime(x['Timestamp']), fcst['p10']))\n",
    "p10_fcst = list(map(lambda x: x['Value'], fcst['p10']))\n",
    "p50_fcst = list(map(lambda x: x['Value'], fcst['p50']))\n",
    "p90_fcst = list(map(lambda x: x['Value'], fcst['p90']))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(time_stamp, p50_fcst)\n",
    "plt.fill_between(time_stamp, p10_fcst, p90_fcst, alpha=0.2)\n",
    "plt.title(\"DeepAR Forecast 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Exporting your Forecasts<a class=\"anchor\" id=\"export\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f's3://{bucketName}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_name_deep_ar = f'{project}_cold_start_forecast_export_deep_ar_plus'\n",
    "forecast_export_name_deep_ar_path = f\"{s3_data_path}/{forecast_export_name_deep_ar}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_export_response_deep_ar = forecast.create_forecast_export_job(ForecastExportJobName=forecast_export_name_deep_ar,\n",
    "                                                        ForecastArn=forecast_arn_deep_ar_2,\n",
    "                                                        Destination={\n",
    "                                                            \"S3Config\" : {\n",
    "                                                                \"Path\": forecast_export_name_deep_ar_path,\n",
    "                                                                \"RoleArn\": role_arn\n",
    "                                                            }\n",
    "                                                        })\n",
    "forecast_export_arn_deep_ar = create_forecast_export_response_deep_ar['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast_export_job(ForecastExportJobArn = forecast_export_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9. Cleaning up your Resources<a class=\"anchor\" id=\"cleanup\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the above steps, we can start to cleanup the resources we created. All delete jobs, except for `delete_dataset_group` are asynchronous, so we have added the helpful `wait_till_delete` function. \n",
    "Resource Limits documented <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/limits.html\">here</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast export\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_deep_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_deep_ar))\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_deep_ar_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete predictor\n",
    "util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn_deep_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series dataset import jobs\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=meta_dataset_import_job_arn))\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=ts_cold_start_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series datasets\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=ts_dataset_arn))\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=meta_dataset_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataset group\n",
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
