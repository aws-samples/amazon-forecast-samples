{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Clustering using DTW KMeans\n",
    "\n",
    "In this notebook, we will train a KMeans Clustering algorithm based on DTW distances between Time Series data. \n",
    "\n",
    "We leverage the [tslearn library](https://tslearn.readthedocs.io/en/stable/index.html) for clustering. The data used in this analysis is publicly available via UCI Archive under [Online Retail II Data Set](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II).\n",
    "\n",
    "We have cleaned and preprocessed this dataset in the optional notebook: 01. Optional - Data Cleaning and Preparation. The reader may directly use the preprocessed data included in the repository under: `./data/df_pivoted.zip` for running this notebook.\n",
    "\n",
    "Tested with Python3, Pandas version 1.0.5.\n",
    "\n",
    "*References*\n",
    " * Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    " * Direct link to UCI dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    " * tslearn github repo: https://github.com/tslearn-team/tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - suppress warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = pd.read_csv('./data/df_pivoted.zip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data to laod to tslearn time_series_dataset object\n",
    "df_pivoted.set_index('StockCode', inplace=True)\n",
    "\n",
    "print(df_pivoted.shape, df_pivoted.columns)\n",
    "\n",
    "df_pivoted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# convert dataframe to time_series_dataset\n",
    "X = to_time_series_dataset(df_pivoted.values)\n",
    "\n",
    "# normalize time series to zero mean and unit variance\n",
    "X_train = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "\n",
    "print(X.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create required directory structure\n",
    "dir_paths = ['./tsl', './tsl/models', './tsl/plots']\n",
    "\n",
    "for dir_path in dir_paths:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform clustering\n",
    "\n",
    "With 12 cores, the clustering operation can take roughly half an hour. Your mileage may vary depending on your machine's configuration. `n_jobs = -1` ensures that the training uses all available cores on your machine.\n",
    "\n",
    "Other alternative to the distance metric is \"Soft-DTW\" which may produce higher separation at a higher compute cost. Please see the `tslearn` documentation [link](https://tslearn.readthedocs.io/en/stable/auto_examples/clustering/plot_kmeans.html#sphx-glr-auto-examples-clustering-plot-kmeans-py) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# algorithm configuration\n",
    "algo = \"DTW_kmeans\"\n",
    "metric = \"dtw\"\n",
    "\n",
    "# cluster configuration\n",
    "N_CLUSTERS = 3\n",
    "\n",
    "model= TimeSeriesKMeans(n_clusters=N_CLUSTERS,\n",
    "                        metric=metric,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=0)\n",
    "\n",
    "y_pred = model.fit_predict(X_train)\n",
    "\n",
    "model.to_pickle(f\"./tsl/models/{algo}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup clustering results\n",
    "np.save(f\"./data/tls_{algo}_cluster_labels\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the different clusters to visually inspect the homogeneity of the cluster composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for yi in range(N_CLUSTERS):\n",
    "    X_sub = X_train[y_pred == yi]\n",
    "    ts_cnt = pd.Series(y_pred[y_pred == yi]).shape[0]\n",
    "    fig = plt.figure()\n",
    "    plt.title(f\"{algo} | Cluster ID: {yi} | TS Count: {ts_cnt}\")\n",
    "    for xx in X_sub:\n",
    "        plt.plot(xx.ravel(), color='xkcd:sky blue', alpha=0.025)\n",
    "    fig.savefig(f\"./tsl/plots/{algo}_cls_lbl_{yi}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TTS for different clusters\n",
    "\n",
    "We can now split the TTS into clusters based on the labels for the different items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to TTS format expected by Forecast service\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "df_tts = pd.melt(df_pivoted, id_vars=['StockCode'])\n",
    "df_tts.columns = ['item_id', 'timestamp', 'target_value']\n",
    "df_tts['timestamp'] = df_tts['timestamp'].str[:10]  # keep only the date part\n",
    "\n",
    "print(df_tts.shape, df_tts.dtypes)\n",
    "\n",
    "df_tts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Hold-out Split\n",
    "\n",
    "Hold-out set offers a way for verifying model performance on unseen data. With this dataset, we are looking to forecast a week out (Forecast Horizon = 1 Week) and therefore leave out a week worth of data out from the TTS as holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_tts['timestamp']), max(df_tts['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_tts[df_tts['timestamp'] < '2010-12-03']\n",
    "df_test = df_tts[df_tts['timestamp'] > '2010-12-02']\n",
    "\n",
    "df_tts.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we have adequate coverage across train and test\n",
    "df_tts.item_id.nunique(), df_train.item_id.nunique(), df_test.item_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if restarting, reload the cluster labels\n",
    "y_pred = np.load(f\"./data/tls_{algo}_cluster_labels.npy\")\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup dataframe with item_ids and corresponding labels\n",
    "df_lbl = pd.DataFrame()\n",
    "df_lbl['item_id'] = df_pivoted['StockCode']\n",
    "df_lbl['label'] = y_pred\n",
    "\n",
    "df_lbl.shape, df_lbl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge labels back to the TTS\n",
    "df_mrg = df_train.merge(df_lbl, how='left')\n",
    "\n",
    "print(df_mrg.shape, df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrg.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create required directory structure\n",
    "dir_paths = ['./train', './train/cls_01', './train/cls_02', './train/cls_03']\n",
    "\n",
    "for dir_path in dir_paths:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and save TTS\n",
    "record_count = 0\n",
    "for i in range(N_CLUSTERS):\n",
    "    df_tmp = df_mrg[['item_id', 'timestamp', 'target_value']][df_mrg['label']==i]\n",
    "    df_tmp.to_csv(f\"./train/cls_0{i+1}/tts_{i+1}.csv\", header=None, index=None)\n",
    "    record_count += df_tmp.shape[0]\n",
    "    \n",
    "print(record_count, df_mrg.shape[0])  # verify that all time series are retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Complete\n",
    "\n",
    "These TTS files can now be uploaded to S3 and used to train Forecast models as described in the [Forecast Developers Guide](https://docs.aws.amazon.com/forecast/latest/dg/what-is-forecast.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
