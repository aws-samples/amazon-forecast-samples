{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute forecast metrics using item-level backtests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DC Bikeshare Rentals data</b>\n",
    "\n",
    "Our goal is to predict the number of DC Bikeshare rentals in the next 24 hours for each of 467 rental locations.  To do this, we used Amazon Forecast (with Target Time Series, i.e. historical demand only) to create baseline forecasts with 1 hour frequency and 1 week forecast horizon.  For this demo, we did not use any metadata or custom features related data, which is usually done as a next step to get improved accuracy.\n",
    "<ul>\n",
    "    <li>See <a href=\"https://aws.amazon.com/blogs/machine-learning/measuring-forecast-model-accuracy-to-optimize-your-business-objectives-with-amazon-forecast/\", target='_blank'>blog post for screens how the forecast was created.</a></li>\n",
    "<li>Original data source: <a href=\"https://www.capitalbikeshare.com/system-data\", target='_blank'> https://www.capitalbikeshare.com/system-data</a> </li>\n",
    "    </ul>  \n",
    "\n",
    "This notebook will show how you can validate your Predictor (validate model train) before deploying the Predictor to make Forecasts.  Generally, in machine learning, you want to validate a model on train/validation data before deciding to deploy a model to make inferences in production.  In the overall Amazon Forecast workflow, this notebook covers <i>step 6. Inspect the model using the backtest window forecasts, see context below.</i>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<b>Overall process for using Amazon Forecast:</b>\n",
    "\n",
    "![\"Amazon Forecast overview workflow\"](images/forecast_steps_overview.png \"Amazon Forecast overview workflow\")\n",
    "\n",
    "<ol>\n",
    "    <li>Prepare your data and save up to 3 separate .csv files.  In Forecast there are  3 types of Datasets (Target, Related, and Meta data). <br>\n",
    "        <ul>\n",
    "        <li>The Target Time Series is required, it is the historical values of what you're trying to predict, i.e. historical y-values or target values.  The others provide additional context with certain algorithms. <br>\n",
    "        </ul><br>\n",
    "    <li>Per dataset, you will specify the schema. </li>\n",
    "    <li>Per dataset, create a Data Import Job. Give the S3 location where data will be read from. </li>\n",
    "    <li>Create a Dataset Group.  This is a container that groups together your models, data they are trained on, and forecasts.  <br>\n",
    "        <ul>\n",
    "            <li>Having this grouping is convenient if in the future, you want to look up artifacts how you ran a particular forecast. </li>\n",
    "        <li>The Dataset Group is also how you can run inferences (forecasts) in the future, without retraining, just by importing new data, by creating a new Data Import job.  </li>\n",
    "        </ul><br>\n",
    "    <li>Train a model.  Amazon Forecast offers AutoML to do this process for you, but you can also select a particular algorithm (6 built-in algorithms).  AutoML will do Hyper Parameter Optimization(HPO) to determine the most performant values automatically, or you can select your own values.</li>\n",
    "    <b><li>Inspect the model using the backtest window forecasts. This notebook focuses on this step.</li>\n",
    "    <ul>\n",
    "    <li>We will use the built-in feature of Amazon Forecast that exports backtest window forecasts together with actuals, for Predictor analysis.  </li>\n",
    "    <li>Predictor evaluation is recommended to make an informed decision whether to deploy the current Predictor to make Forecasts, or whether to fix something in the data setup and train a new Predictor. </li></b>\n",
    "    </ul><br>\n",
    "    <li>Deploy the model (or create a forecast).  Here you are deploying your model so you can use it to do inferences (or generate forecasts).</li>\n",
    "    <li>Query and visualize the Forecast (available in console UI). Spot-check actual values and forecasted values at different quantiles in the console for a particular itemID. The visualization feature in the console is basic, not user-interactive.  For more advanced visualizations, consider exporting your forecasts to an S3 location and point your BI tool (e.g. Tableau or Quicksight) to that data.</li>\n",
    "    </ol>\n",
    "<br>\n",
    "\n",
    "\n",
    "<b>Table Of Contents for task of inspecting the model using the backtest window forecasts</b>\n",
    "* [Set up and install libraries](#setup)\n",
    "* [Export predictor backtests](#export)\n",
    "* [Assemble and read predictor backtest files](#read)\n",
    "* [Demo using item-level forecast files](#demo)\n",
    "* [Visualize backtest window accuracy](#visualizations)\n",
    "* [Calculate custom MAPE](#mape)\n",
    "* [Cleanup](#cleanup)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up  <a class=\"anchor\" id=\"setup\"></a>\n",
    "Import and install Python and aws libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.0.5\n",
      "numpy: 1.19.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "# display all columns wide\n",
    "pd.set_option('display.max_columns', None)\n",
    "# display all rows long\n",
    "pd.set_option('display.max_rows', None)\n",
    "# display horizontal scrollbar for wide columns\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "#turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "import numpy as np\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# Python library for AWS APIs\n",
    "import boto3\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../common\") )\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Function to concat .part files\n",
    "#########\n",
    "\n",
    "def read_backtest_predictions(BUCKET_NAME, s3_path):\n",
    "    \"\"\"Read predictor backtest predictions export files\n",
    "       Inputs: \n",
    "           BUCKET_NAME = S3 bucket name\n",
    "           s3_path = S3 path to Predictor.part files\n",
    "                         , everything after \"s3://BUCKET_NAME/\" in S3 URI path to your .part files\n",
    "       Return: Pandas dataframe with all .part files concatenated row-wise\n",
    "    \"\"\"\n",
    "    # set s3 path\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_bucket = boto3.resource('s3').Bucket(BUCKET_NAME)\n",
    "    s3_depth = s3_path.split(\"/\")\n",
    "    s3_depth = len(s3_depth) - 1\n",
    "    \n",
    "    # set local path\n",
    "    local_write_path = \"visualize\"\n",
    "    !mkdir -p $local_write_path\n",
    "    !rm -rf $local_write_path/*\n",
    "    \n",
    "    # concat part files\n",
    "    part_filename = \"\"\n",
    "    part_files = list(s3_bucket.objects.filter(Prefix=s3_path))\n",
    "    print(f\"Number .part files found: {len(part_files)}\")\n",
    "    for file in part_files:\n",
    "        # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "        if \"csv\" in file.key:\n",
    "            part_filename = file.key.split('/')[s3_depth]\n",
    "            window_object = s3.Object(BUCKET_NAME, file.key)\n",
    "            file_size = window_object.content_length\n",
    "            if file_size > 0:\n",
    "                s3.Bucket(BUCKET_NAME).download_file(file.key, local_write_path+\"/\"+part_filename)\n",
    "        \n",
    "    # Read from local dir and combine all the part files\n",
    "    temp_dfs = []\n",
    "    for entry in os.listdir(local_write_path):\n",
    "        if os.path.isfile(os.path.join(local_write_path, entry)):\n",
    "            df = pd.read_csv(os.path.join(local_write_path, entry), index_col=None, header=0)\n",
    "            temp_dfs.append(df)\n",
    "\n",
    "    # Return assembled .part files as pandas Dataframe\n",
    "    fcst_df = pd.concat(temp_dfs, axis=0, ignore_index=True, sort=False)\n",
    "    return fcst_df\n",
    "\n",
    "\n",
    "#########\n",
    "# Functions to calculate item velocity and classify items a \"slow\" or \"fast\"\n",
    "#########\n",
    "\n",
    "def get_velocity_per_item(df, timestamp_col, item_id_col=\"item_id\"):\n",
    "    \"\"\"Calculate item velocity as item demand per hour.  \n",
    "    \"\"\"\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    max_time_df = df.groupby([item_id_col], as_index=False).max()[[item_id_col, timestamp_col]]\n",
    "    max_time_df.columns = [item_id_col, 'max_time']\n",
    "    \n",
    "    min_time_df = df.groupby([item_id_col], as_index=False).min()[[item_id_col, timestamp_col]]\n",
    "    min_time_df.columns = [item_id_col, 'min_time']\n",
    "    \n",
    "    df = df.merge(right=max_time_df, on=item_id_col)\n",
    "    df = df.merge(right=min_time_df, on=item_id_col)\n",
    "    \n",
    "    df['time_span'] = df['max_time'] - df['min_time']\n",
    "    df['time_span'] = df['time_span'].apply(lambda x: x.seconds / 3600 + 1) # add 1 to include start datetime and end datetime\n",
    "    df = df.groupby([item_id_col], as_index=False).agg({'time_span':'mean', 'target_value':'sum'})\n",
    "    df['velocity'] = df['target_value'] / df['time_span']\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_fast_slow_moving_items_all(gt_df\n",
    "                                   , timestamp_col\n",
    "                                   , target_value_col\n",
    "                                   , item_id_col=\"item_id\"):\n",
    "    \"\"\"Calculate mean velocity over all items as \"criteria\".\n",
    "       Assign each item into category \"fast\" or \"slow\" depending on whether its velocity > criteria.\n",
    "    \"\"\"\n",
    "    gt_df_velocity = gt_df[[item_id_col, timestamp_col, target_value_col]].copy().reset_index(drop=True)\n",
    "    gt_df_velocity = get_velocity_per_item(gt_df_velocity, timestamp_col, item_id_col)\n",
    "    criteria = gt_df_velocity['velocity'].mean()\n",
    "    gt_df_velocity['fast_moving'] = gt_df_velocity['velocity'] > criteria\n",
    "    print('average velocity of all items:', criteria)\n",
    "    \n",
    "    fast_moving_items = gt_df_velocity[gt_df_velocity['fast_moving'] == True][item_id_col].to_list()\n",
    "    slow_moving_items = gt_df_velocity[gt_df_velocity['fast_moving'] == False][item_id_col].to_list()\n",
    "    return fast_moving_items, slow_moving_items\n",
    "\n",
    "\n",
    "###########\n",
    "# Define custom metrics\n",
    "###########\n",
    "\n",
    "def truncate_negatives_to_zero(the_df, target_value_col, quantile_cols):\n",
    "    \"\"\"In case you are expecting positive numbers for actuals and predictions,\n",
    "       round negative values up to zero.\n",
    "       \n",
    "       Be careful that this is acceptable treatment of negatives for your use case.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = the_df.copy()\n",
    "    \n",
    "    for q in quantile_cols:\n",
    "        num_neg_predictions = df[q].lt(0).sum()\n",
    "        print(f\"Num negative {q} predictors: {num_neg_predictions}\")\n",
    "\n",
    "        # replace\n",
    "        df[q] = df[q].mask(df[q] < 0, 0)\n",
    "\n",
    "        # check you did the right thing\n",
    "        num_neg_predictions = df[q].lt(0).sum()\n",
    "        print(f\"Num negative {q} predictors: {num_neg_predictions}\")\n",
    "\n",
    "    # truncate negative actuals\n",
    "    num_neg_actuals = df[target_value_col].lt(0).sum()\n",
    "    print(f\"Num negative actuals: {num_neg_actuals}\")\n",
    "\n",
    "    # replace\n",
    "    df[target_value_col] = df[target_value_col].mask(df[target_value_col] < 0, 0)\n",
    "\n",
    "    # check you did the right thing\n",
    "    num_neg_actuals = df[target_value_col].lt(0).sum()\n",
    "    print(f\"Num negative actuals: {num_neg_actuals}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "       \n",
    "def calc_mape(target, forecast):\n",
    "    \"\"\"Calculates custom mape for a specific quantile and window with formula:\n",
    "            sum(| |predicted| - |actual| | / |actual|)\n",
    "       Input: single numbers for target and forecast\n",
    "       Output: mape = floating point number\n",
    "    \"\"\"\n",
    "    denominator = np.abs(target)\n",
    "    flag = denominator <= 1e-8\n",
    "\n",
    "    mape = np.mean(\n",
    "        (np.abs( np.abs(target) - np.abs(forecast)) * (1.0 - flag)) / (denominator + flag)\n",
    "    )\n",
    "    return mape\n",
    "\n",
    "\n",
    "def quantile_loss(actual, pred, quantile):\n",
    "    \"\"\"Calculate weighted quantile loss for a specific quantile and window\n",
    "       Input: single numbers for actual and forecast\n",
    "       Output:  wql = floating point number\n",
    "    \"\"\"\n",
    "    denom = sum(np.abs(actual))\n",
    "    num = sum([(1-quantile) * abs(y_hat-y) if y_hat > y\n",
    "               else quantile * abs(y_hat-y) for y_hat, y in zip(pred, actual)])\n",
    "    if denom != 0:\n",
    "        return 2 * num / denom\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run Amazon Forecast, you'll need an AWS account.  \n",
    "<b>Make sure you can log in to: https://console.aws.amazon.com/.  </b>  Then read each cell carefully and execute the cells in this notebook.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Configure the S3 bucket name and region name for this lesson.</b>\n",
    "\n",
    "- If you don't have an S3 bucket, create it first on S3.\n",
    "- Although we have set the region to us-west-2 as a default value below, you can choose any of the regions that the service is available in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket name [enter to accept default]: \n",
      "region [enter to accept default]: us-west-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Get user inputs for S3 bucket name and region\n",
    "\n",
    "# TODO: put back default value or user override value\n",
    "default_bucket = \"christy-forecast\"  #default bike-demo\n",
    "BUCKET_NAME = input(\"S3 bucket name [enter to accept default]: \") or default_bucket\n",
    "default_region = 'us-west-2'\n",
    "REGION = input(f\"region [enter to accept default]: {default_region}\") or default_region \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the setup process is to validate that your account can communicate with Amazon Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize forecast session\n",
    "session = boto3.Session(region_name=REGION) \n",
    "forecast = session.client(service_name='forecast') #Amazon Forecast Service api session\n",
    "# forecastquery = session.client(service_name='forecastquery') #Amazon Forecast Query api session - not used here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the setup process is to create an AWS Role with Forecast and S3 permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name christy to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current AWS identity is not a role: arn:aws:iam::625299737718:user/christy, therefore it cannot be used as a SageMaker execution role",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-82d338e4b8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdefault_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Success! Created role arn = {role_arn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   3360\u001b[0m         \u001b[0;34m\"SageMaker execution role\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3361\u001b[0m     )\n\u001b[0;32m-> 3362\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::625299737718:user/christy, therefore it cannot be used as a SageMaker execution role"
     ]
    }
   ],
   "source": [
    "# Create the role to provide to Amazon Forecast.\n",
    "# role_name = \"ForecastNotebookRole\"\n",
    "# print(f\"Creating Role {role_name} ...\")\n",
    "# role_arn = util.get_or_create_iam_role( role_name = role_name )\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "default_role = get_execution_role()\n",
    "\n",
    "print(f\"Success! Created role arn = {role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export predictor backtests <a class=\"anchor\" id=\"export\"></a>\n",
    "\n",
    "\"Backtesting\" is a cross-validation technique for time series that uses multiple train/test splits that keep time order of the data.  Using multiple train-test splits (i.e. more than 1 backtest window) will result in more models being trained, and in turn, a more robust estimate how the model (chosen algorithm and hyperameters) will perform on unseen data.\n",
    "<a href=\"https://docs.aws.amazon.com/forecast/latest/dg/metrics.html#backtesting, target='_blank' \">More details on the Amazon Forecast documentation page.</a>\n",
    "\n",
    "In the next few cells, we ask for your Predictor arn and S3 location where to write the backtest export files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get user inputs for predictor arn and where to export the files\n",
    "\n",
    "# TODO: Clear output of this cell which contains account#\n",
    "# TODO: put back default value or user override value\n",
    "# default = \"arn:aws:forecast:us-west-2:123456789012:predictor/bike_demo_auto\"\n",
    "default_predictor_arn = \"arn:aws:forecast:us-west-2:788825421177:predictor/nyc_taxi_new_with_indra\"  \n",
    "predictor_arn = getpass.getpass(\"weather predictor arn [enter to accept default]: \") \\\n",
    "                            or default_weather_predictor_arn\n",
    "\n",
    "# default = \"s3://bike-demo/forecasts/\"\n",
    "default_export_path = 's3://forecastdemogunjangarg/nyc_export/nyc_taxi_new_with_indra/'\n",
    "export_path = input(f\"weather predictor backtest export path [enter to accept default]:{default_weather_export_path}\") \\\n",
    "                                or default_weather_export_path \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the next few cells, we ask Amazon Forecast to export the Predictor backtest window forecasts via API.  The same could be done by <a href=\"https://aws.amazon.com/blogs/machine-learning/measuring-forecast-model-accuracy-to-optimize-your-business-objectives-with-amazon-forecast/, target='_blank' \">clicking the \"Export backtest results\" button on the Predictor page, as shown in the blog.</a>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Call CreatePredictorBacktestExportJob using predictor Arn and S3 export path\n",
    "\n",
    "backtestExportJobName = 'bike_demo_forecasts'\n",
    "backtest_export_job_response =forecast.create_predictor_backtest_export_job(PredictorBacktestExportJobName=backtestExportJobName,\n",
    "                                                          PredictorArn=predictor_arn,\n",
    "                                                          Destination= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":export_path,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for HTTPStatusCode 200\n",
    "\n",
    "backtest_export_job_arn = backtest_export_job_response['PredictorBacktestExportJobArn']\n",
    "backtest_export_job_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## CHECK STATUS OF YOUR EXPORT JOB - BACKTEST FORECASTS\n",
    "\n",
    "status_indicator = util.StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_predictor_backtest_export_job(PredictorBacktestExportJobArn = \\\n",
    "                        backtest_export_job_response['PredictorBacktestExportJobArn'])['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "# Wait until you see \"ACTIVE\" below...\n",
    "# This will take a while, go get a cup of tea now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API steps you did above, could equivalently be done in the UI by clicking the \"Export backtest results\" button on the Predictor page.  You'll see export job details on the screen.\n",
    "\n",
    "![\"Export backtest results\"](images/export_backtest_results.png \"Export backtest results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble and read predictor backtest files <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "After Forecast Predictor Backtest Export step finishes, you will have a number of .part files within 2 separate folders.  The cell below concatenates all the .part files per folder into a single .csv file which can be saved to an S3 location of your choice. <br>\n",
    "\n",
    "Make sure to change each section below with <b><i>your S3 locations</i></b>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get user inputs for where to find exported files\n",
    "\n",
    "# TODO: put back default value or user override value\n",
    "default_export_path = 's3://bike-demo/bike_share_open_data/export_files/bike_backtest_accuracies/'\n",
    "export_path = input(f\"weather predictor backtest export path [enter to accept default]:{default_export_path}\") \\\n",
    "                                or default_export_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble and read backtest forecasts\n",
    "\n",
    "After Forecast Predictor Backtest Export step finishes, you will have a number of .part files within 2 separate folders.  The cell below concatenates all the .part files per folder into a single .csv file which can be saved to an S3 location of your choice. <br>\n",
    "\n",
    "Make sure to change each section below with <b><i>your S3 locations</i></b>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASSEMBLE S3 PATH TO INPUT FILES FROM PREVIOUSLY-ENTERED S3 path where you saved export files\n",
    "\n",
    "# You should already have export_path from inputs above\n",
    "\n",
    "# path to files is everything after BUCKET_NAME/, it should end in \"/\"\n",
    "s3_path_to_files = export_path.split(BUCKET_NAME)[1][1:]\n",
    "print(f\"path to files: {s3_path_to_files}\")\n",
    "# s3://christy-forecast/open-data-analytics-taxi-trips/backtest_exports/forecasted-values/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Concat files in a way that works on any OS\n",
    "\n",
    "if platform == \"Windows\":\n",
    "    print(\"Your system is Windows\")\n",
    "    # copy part files locally\n",
    "    try:\n",
    "        shutil.rmtree(\"tempfcst\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    os.makedirs(\"tempfcst\")\n",
    "    !aws s3 cp $from_files tempfcst/ --recursive --include \"*.csv\"\n",
    "\n",
    "    # Concat .csv part files locally\n",
    "    path = r'tempfcst'\n",
    "    allFiles = glob.glob(path + \"/*.csv\")\n",
    "    with open(temp_file, 'wb') as outfile:\n",
    "        for i, fname in enumerate(allFiles):\n",
    "            with open(fname, 'rb') as infile:\n",
    "                if i != 0:\n",
    "                    infile.readline()  # Throw away header on all but first file\n",
    "                # Block copy rest of file from input to output without parsing\n",
    "                shutil.copyfileobj(infile, outfile)\n",
    "                print(fname + \" has been imported.\")\n",
    "\n",
    "    # copy concatted local .csv file back to S3\n",
    "    !aws s3 cp $temp_file $to_file_forecasts\n",
    "    \n",
    "elif platform == \"Linux\" or platform == \"Darwin\" :\n",
    "    print(\"Your system is Linux\")\n",
    "    # copy part files locally\n",
    "    !mkdir -p tempfcst\n",
    "    !rm -rf tempfcst/*\n",
    "    !aws s3 cp $from_files tempfcst/ --recursive --include \"*.csv\"\n",
    "\n",
    "    # Concat .csv part files locally\n",
    "    !touch $temp_file\n",
    "    !cat tempfcst/*csv > $temp_file\n",
    "\n",
    "    # copy concatted local .csv file back to S3\n",
    "    !aws s3 cp $temp_file $to_file_forecasts\n",
    "else:\n",
    "    print(\"Unidentified operating system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## READ THE ACCURACIES FILE\n",
    "\n",
    "accuracy_df = pd.read_csv(to_file_accuracies)\n",
    "\n",
    "# keep only data rows\n",
    "print(accuracy_df.shape)\n",
    "accuracy_df = accuracy_df.loc[(accuracy_df.backtestwindow_start_time != \"backtestwindow_start_time\"), :].copy()\n",
    "print(accuracy_df.shape)\n",
    "accuracy_df.drop_duplicates(inplace=True)\n",
    "print(accuracy_df.shape)\n",
    "\n",
    "# correct data types\n",
    "accuracy_df.item_id = accuracy_df.item_id.astype(str)\n",
    "accuracy_df['backtestwindow_start_time'] = pd.to_datetime(accuracy_df['backtestwindow_start_time']\n",
    "                                                 , format=\"%Y-%m-%dT%H:%M:%S\", errors='coerce')\n",
    "accuracy_df['backtestwindow_end_time'] = pd.to_datetime(accuracy_df['backtestwindow_end_time']\n",
    "                                                 , format=\"%Y-%m-%dT%H:%M:%S\", errors='coerce')\n",
    "# convert UTC timestamp to timezone unaware\n",
    "accuracy_df['backtestwindow_start_time'] = accuracy_df.backtestwindow_start_time.dt.tz_localize(None)\n",
    "accuracy_df['backtestwindow_end_time'] = accuracy_df.backtestwindow_end_time.dt.tz_localize(None)\n",
    "\n",
    "# correct dtypes\n",
    "for q in accuracy_df.iloc[:, -4:].columns:\n",
    "    accuracy_df[q] = pd.to_numeric(accuracy_df[q], errors='coerce')\n",
    "\n",
    "# check\n",
    "num_items = len(accuracy_df['item_id'].value_counts(normalize=True, dropna=False))\n",
    "print(f\"Num items: {num_items}\")\n",
    "print(\"Backtest Window Start Dates\")\n",
    "print(accuracy_df.backtestwindow_start_time.unique())\n",
    "\n",
    "print(accuracy_df.dtypes)\n",
    "accuracy_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## READ THE FORECASTS FILE\n",
    "\n",
    "df = pd.read_csv(to_file_forecasts, low_memory=False)\n",
    "\n",
    "# correct data types\n",
    "df.item_id = df.item_id.astype(str)\n",
    "df.target_value = pd.to_numeric(df.target_value, errors='coerce')\n",
    "df.timestamp = pd.to_datetime(df.timestamp\n",
    "                                                 , format=\"%Y-%m-%dT%H:%M:%S\", errors='coerce')\n",
    "df['backtestwindow_start_time'] = pd.to_datetime(df['backtestwindow_start_time']\n",
    "                                                 , format=\"%Y-%m-%dT%H:%M:%S\", errors='coerce')\n",
    "df['backtestwindow_end_time'] = pd.to_datetime(df['backtestwindow_end_time']\n",
    "                                                 , format=\"%Y-%m-%dT%H:%M:%S\", errors='coerce')\n",
    "# convert UTC timestamp to timezone unaware\n",
    "df.timestamp = df.timestamp.dt.tz_localize(None)\n",
    "\n",
    "# drop duplicates\n",
    "print(df.shape)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# check\n",
    "num_items = len(df['item_id'].value_counts(normalize=True, dropna=False))\n",
    "print(f\"Num items: {num_items}\")\n",
    "print()\n",
    "print(\"Backtest Window Start Dates\")\n",
    "print(df.backtestwindow_start_time.unique())\n",
    "\n",
    "print(df.dtypes)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo using the item-level forecast files <a class=\"anchor\" id=\"demo\"></a>\n",
    "\n",
    "The rest of this notebook will focus on how to use the item-level forecasts from the Predictor backtest windows. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get quantile columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map column names in your data to expected key words\n",
    "item_id = \"item_id\"\n",
    "target_value = \"target_value\"\n",
    "timestamp = \"timestamp\"\n",
    "location_id = \"item_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"target_value\"\n",
    "# set predictor dimensions from forecast df\n",
    "predictor_cols = ['item_id', 'timestamp', 'rest_no', 'backtestwindow_start_time', 'backtestwindow_end_time']\n",
    "# exclude cols to automatically find quantiles\n",
    "exclude_cols = predictor_cols.copy()\n",
    "exclude_cols.append(target_value)\n",
    "\n",
    "# get quantile columns from forecast dataframe\n",
    "quantile_cols = [c for c in df.columns if c not in exclude_cols] \n",
    "num_quantiles = len(quantile_cols)\n",
    "print(f\"num quantiles: {num_quantiles}\")\n",
    "quantile_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct data types\n",
    "for q in quantile_cols:\n",
    "    df[q] = pd.to_numeric(df[q], errors='coerce')\n",
    "\n",
    "print(df.dtypes)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before calling error calcs, truncate negative actuals and predictions to 0\n",
    "If you are not expecting negatives, such as for counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Before calling error calcs, truncate negative actuals and predictions to 0\n",
    "\n",
    "df_eligible = df.copy()\n",
    "df_eligible = truncate_negatives_to_zero(df_eligible\n",
    "                                         , target_value_col=target_value\n",
    "                                         , quantile_cols=quantile_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week for convenience\n",
    "df_eligible['day_of_week'] = df_eligible.timestamp.dt.day_name()\n",
    "print(df_eligible.day_of_week.value_counts())\n",
    "\n",
    "# Add window number for convenience\n",
    "windows = df_eligible.backtestwindow_start_time.value_counts().rename_axis('backtestwindow_start_time').reset_index(name='count')\n",
    "windows.sort_values('backtestwindow_start_time', inplace=True)\n",
    "windows.reset_index(inplace=True, drop=True)\n",
    "windows.drop('count', axis=1, inplace=True)\n",
    "windows['window'] = windows.index + 1\n",
    "\n",
    "print(df_eligible.shape)\n",
    "df_eligible = df_eligible.merge(windows, how=\"left\", on=\"backtestwindow_start_time\")\n",
    "print(df_eligible.shape)\n",
    "df_eligible.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations of Backtest Windows\n",
    "\n",
    "Below is 1 chart per item, for 5 random items in the \"fast\" item group.  Y-axis is Actuals and color-coded Forecasts at each quantile.  X-axis is time, starting from the first Backtest Window and ending with the last Backtest Window.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get an x-range of dates of your data\n",
    "\n",
    "print(df_eligible.backtestwindow_start_time.min())\n",
    "print(df_eligible.backtestwindow_start_time.max())\n",
    "\n",
    "x = pd.date_range(start=df_eligible.backtestwindow_start_time.min()\n",
    "                    , end=df_eligible.backtestwindow_start_time.max() + timedelta(days=1), freq='D')\n",
    "x = list(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select random \"fast\" items\n",
    "# random_items = df_eligible.loc[(df_eligible.velocity==\"fast\"), ['item_id']].copy()\n",
    "# random_items = random_items.item_id.value_counts(dropna=False).index.tolist()\n",
    "# random_items = random.sample(random_items, 5)\n",
    "# print(len(random_items))\n",
    "\n",
    "# instead of random fast, choose some fixed examples\n",
    "random_items = [\"31519\", \"31505\", \"31519\", \"31639\", \"31623\"]\n",
    "\n",
    "# gather data for plotting\n",
    "forecasts = df_eligible.iloc[:, -len(quantile_cols)-2:-2]\n",
    "dimension_cols = df_eligible[[item_id, timestamp, target_value]]\n",
    "temp = pd.concat([dimension_cols, forecasts], axis=1)\n",
    "print(temp.shape)\n",
    "# rename \"target_value\" to \"actual_value\" for clearer viz\n",
    "temp.rename(columns={'target_value':'actual_value'}, inplace=True)\n",
    "temp = temp.groupby([timestamp, item_id]).sum()\n",
    "temp.reset_index(inplace=True)\n",
    "temp.set_index(timestamp, inplace=True)\n",
    "print(temp.shape)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize items\n",
    "np.warnings.filterwarnings('ignore')  \n",
    "fig, axs = plt.subplots(len(random_items), 1, figsize=(15, 15), sharex=True)\n",
    "# axx = axs.ravel()\n",
    "\n",
    "\n",
    "for i in range(len(random_items)):\n",
    "    \n",
    "    item = random_items[i]\n",
    "    zoomed = temp.loc[(temp[item_id]==item), :]\n",
    "\n",
    "    zoomed[['actual_value']].plot(ax=axs[i], color='k')\n",
    "    colors = ['mediumpurple', 'orange', 'deepskyblue']\n",
    "    \n",
    "    for j in range(len(quantile_cols)):\n",
    "        quantile = quantile_cols[j]\n",
    "        zoomed[[quantile]].plot(ax=axs[i], color=colors[j])\n",
    "            \n",
    "    axs[i].set_title(f\"Item_id={item}\")\n",
    "    axs[i].set_xlabel(\"Time\")    #date\n",
    "    axs[i].set_ylabel(\"Hourly demand\")   \n",
    "    axs[i].grid(axis='x')\n",
    "    axs[i].set_xticks(x[0:])\n",
    "    axs[i].set_xticklabels([str(dt.date())[0:11] for dt in x[0:]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize items - zoom in to see hours\n",
    "np.warnings.filterwarnings('ignore')  \n",
    "fig, axs = plt.subplots(len(random_items), 1, figsize=(15, 15), sharex=True)\n",
    "\n",
    "\n",
    "for i in range(len(random_items)):\n",
    "    \n",
    "    item = random_items[i]\n",
    "    zoomed = temp.loc[(temp[item_id]==item), :]\n",
    "    zoomed = zoomed['2017-06-20':'2017-06-20']\n",
    "\n",
    "    zoomed[['actual_value']].plot(ax=axs[i], color='k')    \n",
    "    colors = ['mediumpurple', 'orange', 'deepskyblue']\n",
    "    \n",
    "    for j in range(len(quantile_cols)):\n",
    "        quantile = quantile_cols[j]\n",
    "        zoomed[[quantile]].plot(ax=axs[i], color=colors[j])\n",
    "            \n",
    "    axs[i].set_title(f\"Item_id={item}\")\n",
    "    axs[i].set_xlabel(\"Time\")    #date\n",
    "    axs[i].set_ylabel(\"Hourly demand\")   \n",
    "    axs[i].grid(which='minor', axis='x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, peak hours for bicycle rental appear to be between 5-9am and 3-9pm.  For a real customer study, we really should select more data to verify the peak hours...\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo custom item-level accuracy \n",
    "\n",
    "An example customer metric request might be - tell me MAPE for my top-selling items, during peak hours, and tell me this before deploying the Predictor.  Also, please use this MAPE formula.  MAPE = sum( |yhat - y| / |y| ).\n",
    "<br>\n",
    "<br>\n",
    "To tackle this, we'll use the raw actuals, forecasts we just exported from the Predictor backtest windows.  Steps to calculate:\n",
    "<ul>\n",
    "    <li>First, we'll segment the items into \"fast\" and \"slow\" categories, depending on how much demand they have. </li>\n",
    "    <li>Then we'll calculate a custom accuracy MAPE for each group of items.</li>  \n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get \"fast\" vs \"slow\" items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## CALCULATE DEMAND VELOCITY OF ITEMS\n",
    "\n",
    "# categorize items\n",
    "fast_moving_items, slow_moving_items = get_fast_slow_moving_items_all(df_eligible, timestamp, target_value, item_id)\n",
    "\n",
    "# assign item velocity\n",
    "df_eligible['velocity'] = \"slow\"\n",
    "df_eligible.loc[(df_eligible.item_id.isin(fast_moving_items)), 'velocity'] = 'fast'\n",
    "\n",
    "# checkit\n",
    "print(df_eligible.velocity.value_counts(normalize=True, dropna=False))\n",
    "df_eligible.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Display breakdown: how many fast vs slow-moving items\n",
    "\n",
    "total_items_cnt = len(fast_moving_items) + len(slow_moving_items)\n",
    "print(f\"number of fast moving items: {len(fast_moving_items)}, ratio:{len(fast_moving_items) / total_items_cnt}\")\n",
    "\n",
    "print(f\"number of slow moving items: {len(slow_moving_items)}, ratio: {len(slow_moving_items) / total_items_cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to just peak hours\n",
    "Assume peak hours for bicycle rental are Weekdays between 5-9am and 3-9pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Add peak hour flags\n",
    "\n",
    "# add day of week and time of day\n",
    "df_eligible['day_of_week'] = df_eligible[timestamp].dt.day_name()\n",
    "df_eligible['time_of_day'] = df_eligible[timestamp].dt.time\n",
    "\n",
    "# morning commute start and end\n",
    "mc_s = pd.to_datetime('05:00:00').time()\n",
    "mc_e = pd.to_datetime('10:00:00').time()\n",
    "\n",
    "# evening commute start and end\n",
    "ec_s = pd.to_datetime('15:00:00').time()\n",
    "ec_e = pd.to_datetime('21:00:00').time()\n",
    "\n",
    "# initialize flags to zero\n",
    "df_eligible['peak_flag'] = 0\n",
    "df_eligible['weekend_flag'] = 0\n",
    "\n",
    "# add weekend flag\n",
    "df_eligible['weekend_flag'] = df_eligible[timestamp].dt.dayofweek\n",
    "df_eligible['weekend_flag'] = (df_eligible['weekend_flag'] >= 5).astype(int)\n",
    "\n",
    "# add morning commute\n",
    "df_eligible.loc[( (df_eligible.weekend_flag==0)\n",
    "                    & ((df_eligible['time_of_day'] <= mc_e) \n",
    "                        & (df_eligible['time_of_day'] >= mc_s)) ), 'peak_flag'] = 1\n",
    "\n",
    "# add evening commute\n",
    "df_eligible.loc[( (df_eligible.weekend_flag==0)\n",
    "                    & ((df_eligible['time_of_day'] <= ec_e) \n",
    "                        & (df_eligible['time_of_day'] >= ec_s)) ), 'peak_flag'] = 1\n",
    "\n",
    "# check you did the right thing\n",
    "# df_eligible.sample(70).sort_values('time_of_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Restrict evaluation to just peak hours\n",
    "\n",
    "print(df_eligible.shape)\n",
    "df_eligible = df_eligible.loc[(df_eligible.peak_flag==1), :].copy()\n",
    "print(df_eligible.shape)\n",
    "df_eligible.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate custom metric MAPE per quantile for the \"fast\" item group <a class=\"anchor\" id=\"mape\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CALCULATE MAPE PER QUANTILE ACROSS ALL BACKTEST WINDOWS FOR FAST GROUPS OF ITEMS\n",
    "\n",
    "from collections import defaultdict\n",
    "mape_by_moving_fast = dict()\n",
    "mape_fast = []\n",
    "\n",
    "# FAST ITEMS\n",
    "windows_list = list(windows.window)\n",
    "for q in quantile_cols:\n",
    "    quantile_list = []\n",
    "    for w in windows_list:\n",
    "        temp = df_eligible.loc[((df_eligible.velocity==\"fast\")\n",
    "                       & (df_eligible.window==w)), [target_value, q]].copy()\n",
    "        mape_by_moving_fast[q,w] = temp.apply(lambda row: calc_mape(row[target_value], row[q]), axis=1)\n",
    "        \n",
    "        quantile_list.append(mape_by_moving_fast[q,w])\n",
    "    mape_fast.append(np.mean(quantile_list))\n",
    "        \n",
    "mape_fast = pd.DataFrame(mape_fast).T\n",
    "mape_fast.columns = quantile_cols\n",
    "mape_fast.index.name = 'MAPE'\n",
    "print(\"Fast Item Custom MAPE per quantile\")\n",
    "mape_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOK UP STANDARD METRICS FOR THE GROUP OF \"FAST\" ITEMS\n",
    "\n",
    "fast_metrics = accuracy_df.loc[(accuracy_df.item_id.isin(fast_moving_items)), :].copy()\n",
    "# drop the summary row\n",
    "fast_metrics = fast_metrics.loc[(fast_metrics.backtest_window != \"Summary\"), :].copy()\n",
    "# calc mean of the standard metrics across 5 backtest windows\n",
    "fast_metrics = fast_metrics.mean()\n",
    "fast_metrics['item_id'] = \"mean\"\n",
    "\n",
    "print(\"Fast Item Mean Standard Metrics per quantile\")\n",
    "fast_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about \"Accuracy\".  Tech people tend to talk about errors since that is what is calculated.  Depending on your background, errors have different terms.\n",
    "<ul>\n",
    "    <li>Statisticians refer to errors as \"Residuals\". </li>\n",
    "    <li>Machine learning folks refer to errors as \"Loss\". </li>\n",
    "    <li>Business people tend to talk about \"Accuracy\", which is 100% - error rate. </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERT CUSTOM MAPE TO \"ACCURACY\"\n",
    "\n",
    "print(\"Fast Item Custom MAPE per quantile\")\n",
    "100 - mape_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERT STANDARD ERROR METRICS FOR THE GROUP OF \"FAST\" ITEMS TO \"ACCURACY\"\n",
    "\n",
    "print(\"Fast Item Accuracy per quantile\")\n",
    "(1.0 - fast_metrics) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup <a class=\"anchor\" id=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_backtest_export_job_response = \\\n",
    "    forecast.delete_predictor_backtest_export_job(PredictorBacktestExportJobArn = backtest_export_job_arn)\n",
    "delete_backtest_export_job_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
