{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data input for Amazon Forecast\n",
    "\n",
    "Forecasting is used in a variety of applications and business use cases: For example, retailers need to forecast the sales of their products to decide how much stock they need by location, Manufacturers need to estimate the number of parts required at their factories to optimize their supply chain, Businesses need to estimate their flexible workforce needs, Utilities need to forecast electricity consumption needs in order to attain an efficient energy network, and enterprises need to estimate their cloud infrastructure needs.\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/forecast_overview.png\" width=\"98%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook provides a template of typical steps required to prepare data input for Amazon Forecast. So, this is part of \"Upload your data to Amazon Forecast\" in the diagram above.  Your data may need additional Prepare steps and/or you may not need all the steps in this notebook.<br>\n",
    "\n",
    "<b>Step 2 Make forecast choices</b>. You need to make some decisions.  How many time units in future do you want to make forecasts?  E.g. if time unit is Hour, then if you want to forecast out 1 week that would be 24*7 = 168 hours.  You'll also be asked if you want integer or float target_values.  And if 0's really mean 0 or are they missing data?\n",
    "\n",
    "<b>Step 10 Aggregate based on the time granularity of your data.</b> Possible aggregations are minute, hour, day, week, month.  See <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment\" target=\"_blank\">documenatation. </a>\n",
    "\n",
    "<b>Step 12 Visualization time series and validate the time granularity you chose makes sense.</b>  Too low-granularity means the time series looks like white noise and the data could benefit by aggregation at higher-time-level.  For example, you might start to notice time series cyclical patterns when aggregating historical sales by-hour instead of of by-minute.  Alternate between Step 7 Visualization and Step 5 Aggregation until you are happy with the chosen time-grandularity for your forecasts.\n",
    "\n",
    "<b>Steps 10 (Aggregate) and 13 (Split train) include a set of common checks on your data for forecasting.</b> One common mistake is customers think they should \"try Amazon Forecast with a small sample of data\".  If the majority of your time series have fewer than 300 data points, Amazon Forecast will fail, because Amazon Forecast is designed for deep-learning AI forecasting algorithms, which require many data points (typically 1000+) per time series.  Small data is better off using traditional, open-source forecast methods such as ETS (available in Excel), ARIMA, or Prophet.  Small data is not a good fit for Amazon Forecast.  If you really want to try Amazon Forecast, but your data fails the Error Checks consider: \n",
    "\n",
    "<ul>\n",
    "    <li>Can you get more data such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you combine items into fewer item_ids such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you reduce forecast dimensions, such as use item_id only and drop location_id?  If so, return to Step 5.</li>\n",
    "    <li>Can you drop to a lower time-frequency without your data turning into white noise?  Check your data again using Step 10 Aggregate and Step 12 Visualize. </li>\n",
    "</ul>\n",
    " \n",
    "It may be that you find your data is not a good fit for Amazon Forecast.  In that case, it's better that you discover this early.\n",
    "\n",
    "<b>In steps 14-21, we will save headerless Target Time Series (TTS), Item metadata (IM), Related Time Series (RTS) to S3</b> so we can trigger Amazon Forecast automation, see\n",
    "    <a href=\"https://github.com/aws-samples/amazon-forecast-samples/tree/master/workshops/CloudFormationAutomation\" target=\"_blank\">Workshop Instructions Install Automation and Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  bucket steps so list doesn't look so long! <br>\n",
    "TODO:  Add data normalization step\n",
    "\n",
    "# Table of Contents \n",
    "* Step 0: [Set up and install libraries](#setup)\n",
    "* Step 1: [Read data](#read)\n",
    "* Step 2: [Make forecast choices](#choices)\n",
    "* Step 3: [Correct dtypes](#fix_dtypes)\n",
    "* Step 4: [Drop null item_ids](#drop_null_items)\n",
    "* Step 5: [Drop null timestamps](#drop_null_times) \n",
    "* Step 7: [Inspect and treat extremes](#treat_extremes) \n",
    "* Step 8: [Optional - Round negative targets up to 0](#round_negatives) \n",
    "* Step 9: [Optional - Convert negative targets to_nan](#negatives_to_nan) \n",
    "* Step 10: [Aggregate at chosen frequency](#groupby_frequency) \n",
    "* Step 11: [Typical Retail scenario: Find top-moving items](#top_moving_items)\n",
    "* Step 12: [Visualize time series](#visualize) \n",
    "* Step 13: [Classify time series](#Classify)\n",
    "* Step 14: [Split train/test data](#split_train_test)\n",
    "* Step 15: [Prepare and save Target Time Series](#TTS) \n",
    "* Step 16: [Remove time series with no target values at all](#TTS_remove_all0)\n",
    "* Step 17: [Remove time series with end of life](#TTS_remove_end_of_life)\n",
    "* Step 18: [Remove time series with fewer than 5 data points](#TTS_remove_too_few_data_points)\n",
    "* Step 19: [Optional - Assemble and save TTS_sparse, TTS_dense](#TTS-dense_sparse)\n",
    "* Step 20: [Optional - Assemble and save TTS_top, TTS_slow](#TTS_top)\n",
    "* Step 21: [Optional - Assemble and save TTS_regular, TTS_erratic, TTS_intermittent, TTS_lumpy](#TTS_classes)\n",
    "* Step 22: [Assemble and save metadata (if any)](#IM) \n",
    "* Step 23: [Assemble and save RTS (if any)](#RTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used in these notebooks: NYC Taxi trips open data\n",
    "\n",
    "Given hourly historical taxi trips data for NYC, your task is to predict #pickups in next 7 days, per hour and per pickup zone.  <br>\n",
    "\n",
    "<ul>\n",
    "<li>Original data source:  <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\"> https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a> </li>\n",
    "<li>AWS-hosted public source:  <a href=\"https://registry.opendata.aws/nyc-tlc-trip-records-pds/\" target=\"_blank\">https://registry.opendata.aws/nyc-tlc-trip-records-pds/ </a> </li>\n",
    "<li>AWS managed weather data ingestion as a service that is bundled with Amazon Forecast, aggregated by location and by hour.  Initially only for USA and Europe, but depending on demand, possibly in the future for other global regions. </li>\n",
    "<li>Data used:  Yellow taxis dates: 2018-12 through 2020-02 to avoid COVID effects </li>\n",
    "</ul>\n",
    "\n",
    " \n",
    "### Features and cleaning\n",
    "Note: ~5GB Raw Data has already been cleaned and joined using AWS Glue (tutorials to be created in future). \n",
    "<ul>\n",
    "    <li>Join shape files Latitude, Longitude</li>\n",
    "    <li>Add Trip duration in minutes</li>\n",
    "    <li>Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations </li>\n",
    "    <li>Drop 2 unknown zones ['264', '265']\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up and install libraries <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard open libraries\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "# display all columns wide\n",
    "pd.set_option('display.max_columns', None)\n",
    "# display horizontal scrollbar for wide columns\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "# display all rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "#turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "import matplotlib as mpl\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "# choose colorblind\n",
    "color_pal = sns.color_palette(\"colorblind\", 6).as_hex()\n",
    "colorblind6 = ','.join(color_pal).split(\",\")\n",
    "\n",
    "# AWS libraries and initialization\n",
    "import boto3\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../notebooks/common\") )\n",
    "import util\n",
    "import util.fcst_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Functions to classify items as \"top movers\" or not\n",
    "# TODO: generaize to handle any number of forecast dimensions, not just location\n",
    "#########\n",
    "\n",
    "def get_time_min_max(the_df, item_id_col, timestamp_col, location_id_col=None):\n",
    "    \"\"\"Calculate min timestamp, max timestamp per item and/or per item-location time series  \n",
    "       Inputs: pandas dataframe with columns: timestamp, target_value, item_id, location_id (optional)\n",
    "       Outputs: pandas dataframe with 2 extra columns \"min_time\" and \"max_time\"\n",
    "    \"\"\"\n",
    "    df = the_df.copy()\n",
    "    \n",
    "    if location_id_col == None:\n",
    "        # get max\n",
    "        max_time_df = \\\n",
    "            df.groupby([item_id_col], as_index=False).max()[[item_id_col, timestamp_col]]\n",
    "        max_time_df.columns = [item_id_col, 'max_time']\n",
    "        # get min\n",
    "        min_time_df = df.groupby([item_id_col], as_index=False).min()[[item_id_col, timestamp_col]]\n",
    "        min_time_df.columns = [item_id_col, 'min_time']\n",
    "        # merge 2 extra columns per item grouping: max and min\n",
    "        df = df.merge(right=max_time_df, on=item_id_col)\n",
    "        df = df.merge(right=min_time_df, on=item_id_col)\n",
    "        \n",
    "    else:\n",
    "        # get max\n",
    "        max_time_df = \\\n",
    "            df.groupby([item_id_col, location_id_col], as_index=False).max()[[item_id_col, location_id_col, timestamp_col]]\n",
    "        max_time_df.columns = [item_id_col, location_id_col, 'max_time']\n",
    "        # get min\n",
    "        min_time_df = df.groupby([item_id_col, location_id_col], as_index=False).min()[[item_id_col, location_id_col, timestamp_col]]\n",
    "        min_time_df.columns = [item_id_col, location_id_col, 'min_time']\n",
    "        # merge 2 extra columns per item grouping: max and min\n",
    "        df = df.merge(right=max_time_df, on=[item_id_col, location_id_col])\n",
    "        df = df.merge(right=min_time_df, on=[item_id_col, location_id_col])\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_velocity_per_item(the_df, timestamp_col, target_value_col, item_id_col, location_id_col=None):\n",
    "    \"\"\"Calculate velocity as target_demand per time unit per time series\n",
    "       Inputs: pandas dataframe with columns: timestamp, target_value, item_id, location_id (optional)\n",
    "       Outputs: pandas dataframe with extra \"velocity\" column\n",
    "    \"\"\"\n",
    "    df = the_df.copy()\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # append 2 extra columns per time seres: min_time, max_time\n",
    "    if location_id_col == None:\n",
    "        df = get_time_min_max(the_df, item_id_col, timestamp_col)\n",
    "    else:\n",
    "        df = get_time_min_max(the_df, item_id_col, timestamp_col, location_id_col)\n",
    "        \n",
    "#     print (df.sample(10))\n",
    "    \n",
    "    # calculate time span per time seres\n",
    "    df['time_span'] = df['max_time'] - df['min_time']\n",
    "    df['time_span'] = df['time_span'].apply(lambda x: x.seconds / 3600 + 1) # add 1 to include start datetime and end datetime\n",
    "    \n",
    "    # calculate average item demand per time unit\n",
    "    if location_id_col == None:\n",
    "        df = df.groupby([item_id_col], as_index=False).agg({'time_span':'mean', target_value_col:'sum'})\n",
    "    else:\n",
    "        df = df.groupby([item_id_col, location_id_col], as_index=False).agg({'time_span':'mean', target_value_col:'sum'})\n",
    "    df['velocity'] = df[target_value_col] / df['time_span']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_top_moving_items(the_df\n",
    "                           , timestamp_col\n",
    "                           , target_value_col\n",
    "                           , item_id_col\n",
    "                           , location_id_col=None):\n",
    "    \"\"\"Calculate mean velocity over all items as \"criteria\".\n",
    "       Where velocity is demand per time unit per time series\n",
    "       Assign each item into category \"top\" or not depending on whether its velocity > criteria.\n",
    "       Return 4 dataframes:  top_moving and slow_moving items; top_moving and slow_moving time series\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate item-level velocity\n",
    "    df_items_velocity = the_df.copy().reset_index(drop=True)\n",
    "    df_items_velocity = get_velocity_per_item(df_items_velocity, timestamp_col\n",
    "                                           , target_value_col, item_id_col)\n",
    "    # calculate ts-level velocity\n",
    "    if location_id_col is not None:\n",
    "        df_ts_velocity = the_df.copy().reset_index(drop=True)\n",
    "        df_ts_velocity = get_velocity_per_item(df_ts_velocity, timestamp_col\n",
    "                                               , target_value_col, item_id_col, location_id_col)\n",
    "    else:\n",
    "        print(\"No location found\")\n",
    "        df_ts_velocity = pd.DataFrame()\n",
    "    \n",
    "    #########\n",
    "    # define cut=off criteria for \"top-moving\" vs \"slow-moving\" time series\n",
    "    if location_id_col is None:\n",
    "        criteria = df_items_velocity['velocity'].mean()\n",
    "    else:\n",
    "        criteria = df_ts_velocity['velocity'].mean()\n",
    "        # define top-moving if velocity > criteria\n",
    "        df_ts_velocity['top_moving'] = df_ts_velocity['velocity'] > criteria\n",
    "        print('average velocity of all items:', criteria)\n",
    "    #########\n",
    "    \n",
    "    # define top-moving if velocity > criteria\n",
    "    df_items_velocity['top_moving'] = df_items_velocity['velocity'] > criteria\n",
    "    \n",
    "    return df_items_velocity, df_ts_velocity\n",
    "\n",
    "\n",
    "#########\n",
    "# Function to classify time series\n",
    "#########\n",
    "\n",
    "def calc_ADI(theDF, time_col, item_col, target_col):\n",
    "    \"\"\"    \n",
    "    Calculates class of time series\n",
    "    Input: theDF = pandas dataframe already grouped as single item at a time\n",
    "    Ouptu: pandas dataframe with 2 extra columns 'ADI' and 'ts_type'\n",
    "    Rules:\n",
    "    1. Regular demand (ADI < 1.32 and CV² < 0.49). \n",
    "    2. Intermittent demand (ADI >= 1.32 and CV² < 0.49). \n",
    "    3. Erratic demand (ADI < 1.32 and CV² >= 0.49). \n",
    "    4. Lumpy demand (ADI >= 1.32 and CV² >= 0.49). Unforecastable.\n",
    "    \"\"\"\n",
    "    df = theDF.copy()\n",
    "    \n",
    "    # calculate ADI\n",
    "    num_periods = df[time_col].nunique()\n",
    "    num_demands_missing = df[target_col].isna().sum()\n",
    "    num_demands = num_periods - num_demands_missing\n",
    "    ADI = num_periods / num_demands\n",
    "    df['ADI'] = ADI\n",
    "    \n",
    "    # calculate CV, coefficient of variation = std of population / mean value of population\n",
    "    CV = df.loc[(df[target_col]>0), target_col].std() / df.loc[(df[target_col]>0), target_col].mean()\n",
    "    df['CV'] = CV\n",
    "    \n",
    "    # apply rules\n",
    "    df['ts_type'] = 'regular'\n",
    "    if ((ADI < 1.32) & (CV < 0.49)):\n",
    "        df['ts_type'] = 'regular'\n",
    "    elif ((ADI >= 1.32) & (CV < 0.49)):\n",
    "        df['ts_type'] = 'intermittent'     \n",
    "    elif ((ADI < 1.32) & (CV >= 0.49)):\n",
    "        df['ts_type'] = 'erratic'  \n",
    "    elif ((ADI >= 1.32) & (CV >= 0.49)):\n",
    "        df['ts_type'] = 'lumpy'  \n",
    "        \n",
    "#     print(f\"check: item = {df.head(1)[item_col].unique()}, ADI={ADI}, type = {df['ts_type'].unique()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#########\n",
    "# Function to plot time series\n",
    "#########\n",
    "\n",
    "def make_plots(theDF, use_location, sample_items, sample_locations=[]):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # select range to zoom-in on particular time event\n",
    "    zoomed = theDF.copy()\n",
    "    # zoomed = zoomed[\"2019-01-06\":\"2019-12-31\"].copy()\n",
    "    \n",
    "    # separate plot for each item_id\n",
    "    if use_location:\n",
    "        num_plots = len(sample_items) * len(sample_locations)\n",
    "    else:\n",
    "        num_plots = len(sample_items)\n",
    "       \n",
    "    # resize plots smaller if fewer time series to plot\n",
    "    if num_plots >= 5:    \n",
    "        fig, axs = plt.subplots(num_plots, 1, figsize=(15, 15), sharex=True)\n",
    "    elif num_plots > 1:\n",
    "        fig, axs = plt.subplots(num_plots, 1, figsize=(15, 8), sharex=True)\n",
    "    else:\n",
    "        # default at least 2 plots otherwise axs indexing breaks\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "    # fig.subplots_adjust(hspace=0.5) # pad a little if each x-axis has title\n",
    "    \n",
    "    plot_num = 0\n",
    "    for i in range(len(sample_items)):\n",
    "        # plot the item_id's target_value\n",
    "        item = sample_items[i]\n",
    "\n",
    "        # plot item_id, location_id's target value\n",
    "        if use_location:\n",
    "            location = sample_locations[i]\n",
    "            zoomed2 = zoomed.loc[((zoomed[item_id]==item) & (zoomed[location_id]==location)), :].copy()\n",
    "        else:\n",
    "            zoomed2 = zoomed.loc[((zoomed[item_id]==item)), :].copy()\n",
    "\n",
    "        # plot only if time series exists\n",
    "        if zoomed2.shape[0] > 0:\n",
    "\n",
    "            # plot target_value \n",
    "            zoomed2[[target_value]].plot(ax=axs[plot_num])\n",
    "\n",
    "            # set title for each subplot\n",
    "            if use_location:\n",
    "                axs[plot_num].set_title(f\"item {item} and location {location}\")  \n",
    "            else:\n",
    "                axs[plot_num].set_title(f\"item {item}\")  \n",
    "            fig.text(0.04, 0.5, 'Monthly quantity', va='center', rotation='vertical')\n",
    "\n",
    "            # format the x-axis\n",
    "            axs[plot_num].set_xlabel(\"Timestamp\")  # set x-axis title\n",
    "            axs[plot_num].xaxis.label.set_visible(False)\n",
    "\n",
    "            # remove each individual subplot legend\n",
    "            axs[plot_num].get_legend().remove()\n",
    "\n",
    "            # format the grid\n",
    "            axs[plot_num].grid(False)\n",
    "            axs[plot_num].grid(which='major', axis='x')\n",
    "\n",
    "            # increment plot counter\n",
    "            plot_num = plot_num + 1\n",
    "\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create a new S3 bucket for this lesson</b>\n",
    "- The cell below will create a new S3 bucket with name ending in \"forecast-demo-taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique S3 bucket for saving your own data\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# create unique S3 bucket for saving your own data\n",
    "bucket_name = account_id + '-forecast-demo-taxi'\n",
    "if util.create_bucket(bucket_name, region=region):\n",
    "    print(f\"Success! Created bucket {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check you can communicate with Forecast APIs\n",
    "forecast.list_predictors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create IAM Role for Forecast</b> <br>\n",
    "Like many AWS services, Forecast will need to assume an IAM role in order to interact with your S3 resources securely. In the sample notebooks, we use the get_or_create_iam_role() utility function to create an IAM role. Please refer to \"notebooks/common/util/fcst_utils.py\" for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role to provide to Amazon Forecast.\n",
    "role_name = \"ForecastNotebookRole-Basic\"\n",
    "print(f\"Creating Role {role_name} ...\")\n",
    "role_arn = util.get_or_create_iam_role( role_name = role_name )\n",
    "\n",
    "# echo user inputs without account\n",
    "print(f\"Success! Created role arn = {role_arn.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read data <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "The first thing we're going to do is read the headerless .csv file.  Then we need to identify which columns map to required Amazon Forecast inputs.\n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/nyctaxi_map_fields.png\" width=\"82%\">\n",
    "<br>\n",
    "\n",
    "<b>In order to use Weather Index, you need a geolocation-type column.</b>  The geolocation-type column connects your locations to geolocations, and can be 5-digit postal code or latitude_longitude.  For more details, see:\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.aws.amazon.com/forecast/latest/dg/weather.html\" target=\"_blank\">Link to documentation about geolocations</a></li>\n",
    "    <li><a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-forecast-weather-index-automatically-include-local-weather-to-increase-your-forecasting-model-accuracy/\" target=\"_blank\">Our Weather blog, which shows UI steps.</a></li>\n",
    "</ul>\n",
    "\n",
    "The cell below shows an example reading headerless csv file with lat_lon geolocation column, \"pickup_geolocation\".  The rest of this notebook writes headerless csv files to be able to use automation.  If you are not planning on using the automation solution, .csv files with headers are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cleaned, joined, featurized data from Glue ETL processing\n",
    "df_raw = pd.read_csv(\"s3://amazon-forecast-samples/data_prep_templates/clean_features.csv\"\n",
    "                          , parse_dates=True\n",
    "                          , header=None\n",
    "                          , dtype={0:'str'\n",
    "                                   , 1: 'str'\n",
    "                                   , 2: 'str'\n",
    "                                   , 3:'str'\n",
    "                                   , 4: 'int32'\n",
    "                                   , 5: 'float64'\n",
    "                                   , 6: 'str'\n",
    "                                   , 7: 'str'\n",
    "                                   , 8: 'str'}\n",
    "                          , names=['pulocationid', 'pickup_hourly', 'pickup_day_of_week'\n",
    "                                   , 'day_hour', 'trip_quantity', 'mean_item_loc_weekday'\n",
    "                                   , 'pickup_geolocation', 'pickup_borough', 'binned_max_item'])\n",
    "\n",
    "# drop duplicates\n",
    "print(df_raw.shape)\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "df_raw['pickup_hourly'] = pd.to_datetime(df_raw[\"pickup_hourly\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "print(df_raw.shape)\n",
    "print(df_raw.dtypes)\n",
    "start_time = df_raw.pickup_hourly.min()\n",
    "end_time = df_raw.pickup_hourly.max()\n",
    "print(f\"Min timestamp = {start_time}\")\n",
    "print(f\"Max timestamp = {end_time}\")\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map expected column names\n",
    "item_id = \"pulocationid\"\n",
    "target_value = \"trip_quantity\"\n",
    "timestamp = \"pickup_hourly\"\n",
    "location_id = \"pickup_geolocation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = df_raw.shape \n",
    "original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Make forecast choices <a class=\"anchor\" id=\"choices\"></a>\n",
    "\n",
    "Below, you need to make some choices.  First, you will be asked how you want to treat target values (the values you're forecasting). \n",
    "<ol>\n",
    "    <li><b>Do you want your target values to be floating point numbers or integers?</b>  If you choose floating point numbers, you won't be able to use negative-binomial distribution later in the DeepAR+ algorithm.  If you don't care about that, then just hit enter to accept the default, which is floats. </li>\n",
    "    <li><b>Decide if you want to round 0's to nulls.</b>  If you believe in your data '0' really means '0', then just hit enter to accept the default of do nothing to 0's.  On the other hand, if you think some of your 0's were missing data instead of actual 0's, then opt-in by typing 'yes' to convert 0's to nulls. </li>\n",
    "    <li><b>Decide if you want to automatically impute missing categorical fields?</b> If you think you only have a few missing values, you might impute missing values to be the top category.</li>\n",
    "    <li><b>Decide if you want to replace extreme values with mean?</b>  If you think you only have one or two extreme values it might make sense to replace them. On the other hand, if you have quite a few, that is indication those values are not really extreme.</li>\n",
    "    <li><b>Do you want to generate future RTS that extends into future?</b>  If you set this to True, all data will be used for Training.  If you set it to False, a hold-out of length Forecast Horizon will be used to alculte RTS.</li>\n",
    "    <li><b>How many time units do you want to forecast?</b>. For example, if your time unit is Hour, then if you want to forecast out 1 week, that would be 24*7 = 168 hours, so answer = 168. </li>\n",
    "    <li><b>What is the time granularity for your data?</b>. For example, if your time unit is Hour, answer = \"H\". </li>\n",
    "    <li><b>What is the first date you want to forecast?</b>  Training data will be cut short 1 time unit before the desired first forecast snapshot date.  For example, if the granularity of your data is \"D\" and you want your first forecast to happen on Feb 8, 2019, then the last timestamp for training will be Feb 7, 2019.</li>\n",
    "    <li><b>Think of a name you want to give this experiment</b>, so all files will have the same names.  You should also use this same name for your Forecast DatasetGroup name, to set yourself up for reproducibility. </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECAST SETTINGS\n",
    "\n",
    "# do not round target values to integers\n",
    "target_to_integer = False\n",
    "\n",
    "# replace 0's with nulls\n",
    "replace_all_zeroes_with_null = False\n",
    "\n",
    "# replace extremes with mean of last 3 months\n",
    "replace_extremes_with_mean = False\n",
    "\n",
    "# Create RTS with unknown future data\n",
    "# Note: if you set this to True, all known data will be used for Training\n",
    "# Note: if you set this to False, a hold-out of length Forecast Horizon will be used to calculate RTS\n",
    "create_future_RTS_with_unknown_data = False\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|H|30min|15min|10min|5min|1min$ \n",
    "FORECAST_FREQ = \"H\"\n",
    "\n",
    "# what is your forecast horizon in number time units you've selected?\n",
    "# e.g. if you're forecasting in hours, how many hours out do you want a forecast?\n",
    "FORECAST_LENGTH = 168\n",
    "\n",
    "# What is the first date you want to forecast? \n",
    "# Training data will be cut short 1 time unit before the desired first forecast snapshot date\n",
    "# get snapshot date (date of 1st forecast) as last time minus forecast horizon\n",
    "end_time_train = df_raw.pickup_hourly.max() - relativedelta(hours=FORECAST_LENGTH)\n",
    "snapshot_date = end_time_train.date() + relativedelta(days=1)\n",
    "print(f\"Suggested snapshot date = {snapshot_date}\")\n",
    "SNAPSHOT_DATE = datetime.datetime(2020, 2, 23, 0, 0, 0)  \n",
    "\n",
    "# What name do you want to give this experiment?  \n",
    "# Be sure to use same name for your Forecast Dataset Group name.\n",
    "EXPERIMENT_NAME = \"nyctaxi_demo2\"\n",
    "DATA_VERSION = 7\n",
    "\n",
    "# print some validation back to user\n",
    "# TODO dateutil.relativedelta can't take parameter, how to generalize below?\n",
    "AF_freq_to_dateutil_freq = {\"Y\":\"years\", \"M\":\"hours\", \"W\":\"weeks\", \"D\":\"days\", \"H\":\"hours\"}\n",
    "print(f\"Convert your frequency to python dateutil = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH} {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "# end_time_train = SNAPSHOT_DATE - relativedelta(hours=1) #can't substitute variable here?\n",
    "snapshot_end = SNAPSHOT_DATE + relativedelta(hours=+FORECAST_LENGTH) - relativedelta(hours=1)\n",
    "print(f\"Training data end date = {end_time_train}\")\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")\n",
    "print(f\"Forecast end date = {snapshot_end}\")\n",
    "\n",
    "snapshot_date_monthYear = SNAPSHOT_DATE.strftime(\"%m%d%Y\")\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_NAME}_snap{snapshot_date_monthYear}_{FORECAST_LENGTH}{FORECAST_FREQ}\"\n",
    "print(f\"Experiment name = {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Correct dtypes <a class=\"anchor\" id=\"fix_dtypes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # correct dtypes\n",
    "# df_raw['ReportingMonth'] = pd.to_datetime(df_raw[\"ReportingMonth\"], format=\"%Y-%m-%d\", errors='coerce')\n",
    "# df_raw.RegionId = df_raw.RegionId.astype(str)\n",
    "# df_raw.WarehouseId = df_raw.WarehouseId.astype(str)\n",
    "# df_raw.ProductGroup1Id = df_raw.ProductGroup1Id.astype(str)\n",
    "# df_raw.SKUId = df_raw.SKUId.astype(str)\n",
    "# # Use the new pandas Integer type\n",
    "# df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64').astype(str)\n",
    "# # df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64')\n",
    "# print(df_raw.shape)\n",
    "# print(df_raw.dtypes)\n",
    "\n",
    "# # # check\n",
    "# num_location_id = df_raw[\"WarehouseId\"].nunique()\n",
    "# print()\n",
    "# print(f\"Num locs = {num_location_id}\")\n",
    "# num_item_id = df_raw[\"SKUId\"].nunique()\n",
    "# print(f\"Num items = {num_item_id}\")\n",
    "# num_time_series = df_raw.groupby([\"SKUId\", \"WarehouseId\"]).first().shape[0]\n",
    "# possible_num_time_series = num_location_id * num_item_id\n",
    "# percent_dense = np.round(100 * num_time_series / possible_num_time_series, 0)\n",
    "# print(f\"Num time series found = {num_time_series} out of possible {possible_num_time_series} or {percent_dense}%\")\n",
    "# start_time = df_raw.ReportingMonth.min()\n",
    "# end_time = df_raw.ReportingMonth.max()\n",
    "# print(f\"Start time = {start_time}\")\n",
    "# print(f\"End time = {end_time}\")\n",
    "\n",
    "# df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Drop null item_ids <a class=\"anchor\" id=\"drop_null_items\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = df_raw[item_id].unique()\n",
    "print(f\"Number unique items: {len(templist)}\")\n",
    "print(f\"Number nulls: {pd.isnull(templist).sum()}\")\n",
    "\n",
    "if len(templist) < 20:\n",
    "    print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null item_ids, if any exist\n",
    "if pd.isnull(templist).sum() > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[item_id].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(len(df_raw[item_id].unique()))\n",
    "else:\n",
    "    print(\"No missing item_ids found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Drop null timestamps <a class=\"anchor\" id=\"drop_null_times\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null timestamps\n",
    "templist = df_raw.loc[(df_raw[timestamp].isna()), :].shape[0]\n",
    "print(f\"Number nulls: {templist}\")\n",
    "\n",
    "if (templist < 10) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[timestamp].isna()), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null quantities and dates\n",
    "if templist > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[timestamp].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(df_raw['timestamp'].isna().sum())\n",
    "else:\n",
    "    print(\"No null timestamps found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Inspect (and treat) extremes <a class=\"anchor\" id=\"treat_extremes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many extreme values is considered unusual\n",
    "MAX_EXTREMES = 10\n",
    "EXTREME_VALUE = df_raw[target_value].quantile(0.999999)\n",
    "print(EXTREME_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect extremes\n",
    "templist = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].shape[0]\n",
    "print(f\"Number extremes: {templist}\")\n",
    "\n",
    "if (templist < MAX_EXTREMES) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].set_index([item_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_extremes_with_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme value with treated median last 6 months value or last 3 months if hourly data\n",
    "# TODO - generalize this for many extremes...\n",
    "\n",
    "if replace_extremes_with_mean:\n",
    "\n",
    "    # calculate median value from last 3 months before the extreme\n",
    "    keys = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), [timestamp, item_id, 'day_hour', location_id]].max()\n",
    "    temp = df_raw.loc[((df_raw[item_id]==keys[item_id]) \n",
    "                       & (df_raw[location_id]==keys[location_id])\n",
    "                       & (df_raw['day_hour']==keys['day_hour'])\n",
    "                       & (df_raw[timestamp]<keys[timestamp])\n",
    "                       & (df_raw[timestamp]>keys[timestamp] - relativedelta(months=3))), :] \n",
    "    # make sure you've got 6 values if using 6 months\n",
    "#     assert(temp.shape[0]==6)\n",
    "    # calculate median\n",
    "    replace_extreme = temp[target_value].median() #np.round(temp[target_value].mean(),0)\n",
    "    print(replace_extreme)\n",
    "    # visually check if replaced value looks like median target_value\n",
    "    display(temp)\n",
    "else:\n",
    "    print(\"Do not replace any extreme values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT OVERALL TIME SERIES - TO HELP INSPECT EXTREMES\n",
    "df_raw.plot(x=timestamp, y=target_value, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme value with treated median last 6 months value or last 3 months if hourly data\n",
    "\n",
    "if (replace_extremes_with_mean & (templist < MAX_EXTREMES)):\n",
    "    df_clean = df_raw.copy()\n",
    "    # calculate median\n",
    "    replace_extreme = temp[target_value].median()\n",
    "    print(f\"replacing extreme value {df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), target_value].max()} with {replace_extreme}\")\n",
    "    # make the replacement\n",
    "    df_clean.loc[((df_clean[item_id]==keys[item_id]) \n",
    "                       & (df_clean[location_id]==keys[location_id])\n",
    "                       & (df_clean[timestamp]==keys[timestamp])), target_value] = replace_extreme\n",
    "    print(f\"new value is {df_clean.loc[((df_clean[item_id]==keys[item_id]) & (df_clean[location_id]==keys[location_id]) & (df_clean[timestamp]==keys[timestamp])), target_value].max()}\")\n",
    "    \n",
    "    # Inspect extremes again on clean data\n",
    "    EXTREME_VALUE = df_raw[target_value].quantile(0.999999)\n",
    "    print(EXTREME_VALUE)\n",
    "\n",
    "    templist = df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), :].shape[0]\n",
    "    print(f\"Number extremes: {templist}\")\n",
    "\n",
    "    if (templist < MAX_EXTREMES) & (templist > 0) :\n",
    "        print(df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), :].set_index([item_id]))\n",
    "        \n",
    "    ## PLOT OVERALL TIME SERIES - TO SEE IF YOU DID THE RIGHT THING\n",
    "    df_clean.plot(x=timestamp, y=target_value, figsize=(15, 8))\n",
    "else:\n",
    "    print(\"No extreme values found or do not replace any extremes.\")\n",
    "    df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some memory\n",
    "del df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error if we lost some values\n",
    "assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Optional - Round negative targets up to 0 <a class=\"anchor\" id=\"round_negatives\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check negative values\n",
    "print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL!!  MAKE SURE ROUNDING NEGATIVES UP TO 0 MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# If negative values found, round them up to 0\n",
    "if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "    # Check y-value before cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "\n",
    "    # default negative values in demand to 0\n",
    "    print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "    print()\n",
    "    ts_cols = [target_value]\n",
    "\n",
    "    for c in ts_cols:\n",
    "        df_clean.loc[(df_clean[c] < 0.0), c] = 0.0\n",
    "\n",
    "    # Check y-value after cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "else:\n",
    "    print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Optional - Convert negative targets to nan <a class=\"anchor\" id=\"negatives_to_nan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check negative values\n",
    "# print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "# df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CAREFUL!!  MAKE SURE CHANGING NEGATIVES TO NAN NEGATIVES MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# # If negative values found, round them up to 0\n",
    "# if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "#     # Check y-value before cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "\n",
    "#     # default negative values in demand to 0\n",
    "#     print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "#     print()\n",
    "#     ts_cols = [target_value]\n",
    "    \n",
    "#     print ()\n",
    "\n",
    "#     for c in ts_cols:\n",
    "#         df_clean.loc[(df_clean[c] < 0.0), c] = float('nan')\n",
    "        \n",
    "        \n",
    "\n",
    "#     # Check y-value after cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "# else:\n",
    "#     print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # throws error if we lost some values\n",
    "# assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Aggregate at your chosen frequency <a class=\"anchor\" id=\"groupby_frequency\"></a>\n",
    "\n",
    "Below, we show an example of resampling at hourly frequency by forecast dimensions.  Modify the code to resample at other frequencies.\n",
    "\n",
    "Decide which aggregation-level makes sense for your data, which is a balance between desired aggregation and what the data-collection frequency will support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  Add parameter \"Y\", \"M\", \"W\", \"D\", \"H\", ... same as Forecast frequencies\n",
    "Parameter:  forecast dimensions, assume timestamp, item_id\n",
    "Adjust script so it can run automatically based on user input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CHECK TO SEE IF YOUR TIMESERIES DIMENSIONS ARE CORRECT\n",
    "\n",
    "# checking if there are multiple entries per item_id per timestamp per location\n",
    "df_aux = df_clean.copy().set_index([timestamp, item_id, location_id])\n",
    "\n",
    "duplicates = df_aux.pivot_table(index=[item_id, location_id,timestamp], aggfunc='size')\n",
    "duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "print (duplicates[duplicates[\"NumberPerTS\"]>1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if your timeseries dimensions are correct\n",
    "if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "    print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "    print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "else:\n",
    "    print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your assumed dimensions are not unique, code below is to explore adding a composite column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  inspect what is happening on these repeated items\n",
    "# try:\n",
    "#     df_aux.reset_index(inplace = True)\n",
    "# except Exception as e:\n",
    "#     print (e)\n",
    "\n",
    "# test_aux = df_aux[df_aux[item_id]=='1000655']\n",
    "# test_aux.sort_values(by=[timestamp]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since warehouseId is not unique, create new fake composite column\n",
    "# df_clean['location_id'] = df_clean['WarehouseId'] + 'R' + df_clean['RegionId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # map expected column names\n",
    "# item_id = \"SKUId\"\n",
    "# target_value = \"Quantity\"\n",
    "# timestamp = \"ReportingMonth\"\n",
    "# # location_id = \"WarehouseId\"\n",
    "# location_id = \"location_id\"\n",
    "# year = 'year'\n",
    "# week = 'week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHECK AGAIN TO SEE IF YOUR DATA AGGREGATION ASSUMPTION IS CORRECT\n",
    "\n",
    "# # checking if there are multiple entries per item_id per timestamp per location\n",
    "# df_aux = df_clean.copy().set_index([timestamp, item_id, location_id])\n",
    "\n",
    "# duplicates = df_aux.pivot_table(index=[item_id, location_id,timestamp], aggfunc='size')\n",
    "# duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "# # checking to see if your data aggregation is correct\n",
    "# if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "#     print (duplicates[duplicates[\"NumberPerTS\"]>1].head())\n",
    "#     print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "#     print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "# else:\n",
    "#     print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE BLOCK IS AN EXAMPLE OF Monthly AGGREGATION\n",
    "\n",
    "# ## Add month_year column for easier groupby sum\n",
    "# df_clean['year'] = df_clean[timestamp].dt.year\n",
    "# df_clean['year_month'] = df_clean[timestamp].dt.to_period(\"M\").dt.to_timestamp()\n",
    "# print(df_clean.sample(2))\n",
    "\n",
    "# # check counts per year\n",
    "# print()\n",
    "# print(\"checking counts by year...\")\n",
    "# print(df_clean[year].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# # Aggregate target values at level you'd like\n",
    "# # g_month = df_clean.groupby(['year_month', item_id]).sum()\n",
    "# g_month = df_clean.groupby([timestamp, item_id, location_id]).sum()  #our timestamp already at year_month\n",
    "# g_month = g_month[[target_value]]\n",
    "# g_month.reset_index(inplace=True)\n",
    "# print()\n",
    "# print(\"sample aggregated dataframe:\")\n",
    "# print(g_month.head(2))\n",
    "# print()\n",
    "\n",
    "\n",
    "# # ERROR CHECK: DO YOU HAVE ENOUGH DATA POINTS PER TIME SERIES?\n",
    "# # Deep Learning algorithms require at least 300 data points per time series, best if >1000 per time series.\n",
    "# num_data_points = g_month.groupby([item_id, location_id]).count()[timestamp].max()\n",
    "# if (num_data_points > 300):\n",
    "#     print(f\"Success, min number data points found!! Found {num_data_points} data points which is > 300.\")\n",
    "# else:\n",
    "#     print(f\"Error, too few data points.  Need at least 300, found {num_data_points}. \")\n",
    "    \n",
    "# # If you have too few data points, return to aggregation choice and choose smaller time granularity\n",
    "# #   or, aggregate with fewer dimensions, e.g. product-level only instead of product-and-location.\n",
    "\n",
    "\n",
    "# # INSPECT SPARSITY - Check 300-mark on your x-axis\n",
    "# print(\"You want bulk of distribution to sit after 300-mark on x-axis.\")\n",
    "# print(\"If bulk of distribution is bunched closer to 0, choose smaller time granularity or aggregate item-level only.\") \n",
    "# print(\"If all time series have same number of data points (distribution looks like 1 single bar), you might be able to use Amazon Forecast even though too few data points.\")\n",
    "\n",
    "# # num data points per item\n",
    "# sparsity = g_month.copy()\n",
    "# sparsity = sparsity.groupby([item_id, location_id]).count()\n",
    "# sparsity[[target_value]].hist(bins=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE BLOCK IS AN EXAMPLE OF Hourly AGGREGATION\n",
    "# The sample data shipped with notebook is ideal - all time series have 5856 data points, \n",
    "# which is squarely in the Deep Learning desired data size.\n",
    "\n",
    "# Add in time features\n",
    "# Note: timestamp is already truncated to be hourly\n",
    "agg_freq = \"H\"\n",
    "\n",
    "# aggregate by hour\n",
    "g_hour = df_clean.groupby([timestamp, item_id, location_id]).sum()\n",
    "g_hour = g_hour[[target_value]]\n",
    "g_hour.reset_index(inplace=True)\n",
    "print(g_hour.shape, df_clean.shape)\n",
    "display(g_hour.sample(5))\n",
    "\n",
    "# In chart below, you want bulk of distribution to sit after 300-mark on x-axis.\n",
    "# If bulk of distribution is bunched closer to 0, choose smaller time granularity or aggregate item-level only\n",
    "\n",
    "# INSPECT SPARSITY\n",
    "# num data points per item\n",
    "sparsity = g_hour.copy()\n",
    "sparsity = sparsity.groupby([item_id]).count()\n",
    "sparsity[[target_value]].hist(bins=32);\n",
    "plt.title(\"Count number data points per item\");\n",
    "plt.suptitle(\"In chart below, you want bulk of distribution to sit after 300-mark on x-axis.\")\n",
    "\n",
    "\n",
    "# INSPECT DISTRIBUTION OF TARGET_VALUES\n",
    "dist = g_hour[target_value].describe()\n",
    "print(dist)\n",
    "\n",
    "# In chart below, you want to see the shape of a distribution (counts usually negative-binomial)\n",
    "# You do not want to see a randomly uniform shape\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"In chart below, you want to see a distribution, usually negative-binomial or gaussian.\", y=1.08)\n",
    "# plot distribution of target_value itself\n",
    "sparsity = g_hour.copy()\n",
    "sparsity.hist(ax=axs[0])\n",
    "axs[0].set_title(f\"Distribution of {target_value} at {agg_freq} level\", y=1.08);\n",
    "\n",
    "# zoom-in plot distribution of target_value itself\n",
    "sparsity = g_hour.copy()\n",
    "sparsity = sparsity.loc[(sparsity[target_value]< 2*dist['75%']), :].copy()\n",
    "sparsity.hist(bins=32, ax=axs[1])\n",
    "axs[1].set_title(f\"Distribution of {target_value} up to 75% percentile\", y=1.08);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect some items at tail\n",
    "sparsity.head(2)\n",
    "sparsity.loc[(sparsity[target_value] == dist['max']), :].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"2H\"\n",
    "\n",
    "# aggregate by 2-hour\n",
    "g_2hour = df_clean[[timestamp, item_id, location_id, target_value]].copy()\n",
    "g_2hour.set_index(timestamp, inplace=True)\n",
    "g_2hour = g_2hour.groupby(pd.Grouper(freq=agg_freq)).sum()\n",
    "g_2hour.reset_index(inplace=True)\n",
    "print(g_2hour.shape, g_hour.shape)\n",
    "display(g_2hour.sample(2))\n",
    "\n",
    "# INSPECT DISTRIBUTION OF TARGET_VALUES\n",
    "dist = g_2hour[target_value].describe()\n",
    "print(dist)\n",
    "\n",
    "# In chart below, you want to see the shape of a distribution (counts usually negative-binomial)\n",
    "# You do not want to see a randomly uniform shape\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"In chart below, you want to see a distribution, usually negative-binomial or gaussian.\", y=1.08)\n",
    "# plot distribution of target_value itself\n",
    "sparsity = g_2hour.copy()\n",
    "sparsity.hist(ax=axs[0])\n",
    "axs[0].set_title(f\"Distribution of {target_value} at {agg_freq} level\", y=1.08);\n",
    "\n",
    "# zoom-in plot distribution of target_value itself\n",
    "sparsity = g_2hour.copy()\n",
    "sparsity = sparsity.loc[(sparsity[target_value]< 2*dist['75%']), :].copy()\n",
    "sparsity.hist(bins=32, ax=axs[1])\n",
    "axs[1].set_title(f\"Distribution of {target_value} up to 75% percentile\", y=1.08);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"4H\"\n",
    "\n",
    "# aggregate by 2-hour\n",
    "g_4hour = df_clean[[timestamp, item_id, location_id, target_value]].copy()\n",
    "g_4hour.set_index(timestamp, inplace=True)\n",
    "g_4hour = g_4hour.groupby(pd.Grouper(freq=agg_freq)).sum()\n",
    "g_4hour.reset_index(inplace=True)\n",
    "print(g_4hour.shape, g_hour.shape)\n",
    "display(g_4hour.sample(2))\n",
    "\n",
    "# INSPECT DISTRIBUTION OF TARGET_VALUES\n",
    "dist = g_4hour[target_value].describe()\n",
    "print(dist)\n",
    "\n",
    "# In chart below, you want to see the shape of a distribution (counts usually negative-binomial)\n",
    "# You do not want to see a randomly uniform shape\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"In chart below, you want to see a distribution, usually negative-binomial or gaussian.\", y=1.08)\n",
    "# plot distribution of target_value itself\n",
    "sparsity = g_4hour.copy()\n",
    "sparsity.hist(ax=axs[0])\n",
    "axs[0].set_title(f\"Distribution of {target_value} at {agg_freq} level\", y=1.08);\n",
    "\n",
    "# zoom-in plot distribution of target_value itself\n",
    "sparsity = g_4hour.copy()\n",
    "sparsity = sparsity.loc[(sparsity[target_value]< 2*dist['75%']), :].copy()\n",
    "sparsity.hist(bins=32, ax=axs[1])\n",
    "axs[1].set_title(f\"Distribution of {target_value} up to 75% percentile\", y=1.08);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b> Select the aggregation-level to keep, based on results above.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THE GROUPING YOU SELECTED ABOVE\n",
    "\n",
    "df = g_hour.copy()\n",
    "# save memory\n",
    "del g_hour\n",
    "\n",
    "print(df.shape, df_clean.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change timestamp column mapping if necessary\n",
    "# timestamp = \"year_month\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Typical retail scenarios: Find top-moving items <a class=\"anchor\" id=\"top_moving_items\"></a>\n",
    "\n",
    "Next, we want to drill down and visualize some individual item time series.  Typically customers have \"catalog-type\" data, where only the top 20% of their items are top-movers; the rest of the 80% of items are not top-movers.  For visualization, we want to select automatically some of the top-moving items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(location_id) >0:\n",
    "    use_location = True\n",
    "else:\n",
    "    use_location = False\n",
    "    \n",
    "use_location = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIND TOP-MOVING ITEMS\n",
    "\n",
    "if use_location:\n",
    "    print(\"using location_id\")\n",
    "    df_velocity, df_ts_velocity = get_top_moving_items(df, timestamp, target_value, item_id, location_id)\n",
    "else:\n",
    "    print(\"using only item_id\")\n",
    "    df_velocity, df_ts_velocity = get_top_moving_items(df, timestamp, target_value, item_id, None)\n",
    "\n",
    "# Display breakdown: how many top-moving items\n",
    "if use_location:\n",
    "    num_top = df_ts_velocity.loc[(df_ts_velocity.top_moving==True), :].groupby([item_id, location_id]).first().shape[0]\n",
    "    num_slow = df_ts_velocity.loc[(df_ts_velocity.top_moving==False), :].groupby([item_id, location_id]).first().shape[0]\n",
    "    num_time_series = df.groupby([item_id, location_id]).first().shape[0]\n",
    "    print(f\"number of top moving time series: {num_top}, ratio:{np.round(num_top/num_time_series,2)}\")\n",
    "    print(f\"number of slow moving time series: {num_slow}, ratio: {np.round(num_slow/num_time_series,2)}\")\n",
    "    print()\n",
    "    top_moving_ts = df_ts_velocity.loc[(df_ts_velocity.top_moving==True), :]\n",
    "    slow_moving_ts = df_ts_velocity.loc[(df_ts_velocity.top_moving==False), :]\n",
    "    print(\"Top-moving time series\")\n",
    "    print(top_moving_ts.sort_values('velocity', ascending=False).head(3))\n",
    "    print()\n",
    "    print(\"Slow-moving time series\")\n",
    "    print(slow_moving_ts.sort_values('velocity', ascending=False).head(2))\n",
    "else:\n",
    "    num_top = df_velocity.loc[(df_velocity.top_moving==True), item_id].nunique()\n",
    "    num_slow = df_velocity.loc[(df_velocity.top_moving==False), item_id].nunique()\n",
    "    num_time_series = df[item_id].nunique()\n",
    "    print(f\"number of top moving items: {num_top}, ratio:{num_top/num_time_series}\")\n",
    "    print(f\"number of slow moving items: {num_slow}, ratio: {num_slow/num_time_series}\")\n",
    "    print(\"Top-moving items\")\n",
    "    top_moving_items = df_velocity.loc[(df_velocity.top_moving==True), :].sort_values('velocity', ascending=False).copy()\n",
    "    print(top_moving_items.head(3))\n",
    "    print()\n",
    "    print(\"Slow-moving items\")\n",
    "    slow_moving_items = df_velocity.loc[(df_velocity.top_moving==False), :].sort_values('velocity', ascending=False).copy()\n",
    "    print(slow_moving_items.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of completely random items, select top-moving items\n",
    "if use_location:\n",
    "    random_items = top_moving_ts[[item_id, location_id]].sample(5)\n",
    "    random_locations = list(random_items[location_id])\n",
    "    random_items = list(random_items[item_id])\n",
    "else:\n",
    "    random_items = top_moving_items[item_id].sample(5)\n",
    "    random_items = random_items.reset_index()\n",
    "    random_items = random_items[item_id]\n",
    "    \n",
    "print(random_items)\n",
    "if use_location:\n",
    "    print(random_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_items = list(random_items) + ['79', '135']\n",
    "random_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12. Visualize time series <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE PLOTS OF RANDOM TOP-MOVING ITEMS AND LOCATIONS\n",
    "# We chose top-moving items to avoid visualizing time series that might be empty or very sparse.\n",
    "\n",
    "if use_location:\n",
    "    df_plot = df.loc[(df[item_id].isin(random_items)), [item_id, timestamp, target_value, location_id]].copy()\n",
    "    df_plot = df.loc[(df[location_id].isin(random_locations)), [item_id, timestamp, target_value, location_id]].copy()\n",
    "else:\n",
    "    df_plot = df.loc[(df[item_id].isin(random_items)), [item_id, timestamp, target_value]].copy()  \n",
    "\n",
    "df_plot.set_index(timestamp, inplace=True)\n",
    "df_plot.head(2)\n",
    "\n",
    "# make plots\n",
    "make_plots(df_plot, use_location, random_items, sample_locations=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13. Classify Time Series <a class=\"anchor\" id=\"Classify\"></a>\n",
    "Using definitions given here:  https://frepple.com/blog/demand-classification/ and here: https://support.demandcaster.com/hc/en-us/articles/360043259931-Forecast-Structure<br>\n",
    "Original article:  https://robjhyndman.com/papers/idcat <br>\n",
    "\n",
    "Idea:  Based on demand patterns, time series can be classified into one of 4 classes:  Regular, Intermittent, Erratic, or Lumpy.  If you have more than 1 class of time series in your data, this might suggest more than 1 model for your time series predictions.  \n",
    "\n",
    "Rules:\n",
    "<ol>\n",
    "    <li><b>Regular</b> demand (ADI < 1.32 and CV² < 0.49). Regular in time and in quantity. It is therefore easy to forecast and you won’t have trouble reaching a low forecasting error level. Suggested algorithm: <b>Traditional statistical such as Exponential Smoothing, Prophet, or ARIMA.</b></li>\n",
    "    <li><b>Intermittent demand</b> (ADI >= 1.32 and CV² < 0.49). The demand history shows very little variation in demand quantity but a high variation in the interval between two demands. Though specific forecasting methods tackle intermittent demands, the forecast error margin is considerably higher. Suggested algorithm: <b>Croston smoothing or some newer research approach coupled with adjusted error metric over longer time period.</b></li>\n",
    "    <li><b>Erratic</b> demand (ADI < 1.32 and CV² >= 0.49). The demand has regular occurrences in time with high quantity variations. Your forecast accuracy remains shaky. Suggested algorithm: <b>Deep Learning</b></li>\n",
    "<li><b>Lumpy</b> demand (ADI >= 1.32 and CV² >= 0.49). The demand is characterized by a large variation in quantity and in time. It is actually impossible to produce a reliable forecast, no matter which forecasting tools you use. This particular type of demand pattern is unforecastable. Suggested algorithm: <b>bootstrap</b></li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squarify\n",
    "import squarify\n",
    "\n",
    "# SCRATCH - choose 4 colors\n",
    "colors = colorblind6[0:4]\n",
    "sizes = [40, 30, 5, 25]\n",
    "squarify.plot(sizes, color=colors)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trick:  create dataframe without any missing values using cross-join, faster than resample technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = pd.date_range(start=new_start_date, end=end_date, freq='D')\n",
    "# idx = pd.date_range(start=start_time, end=end_time, freq='MS')\n",
    "idx = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "all_times = pd.DataFrame(index=idx)\n",
    "print (f\"Number of data points: {len(all_times.index)}\")\n",
    "print (f\"Start date = {all_times.index.min()}\")\n",
    "print (f\"End date = {all_times.index.max()}\")\n",
    "\n",
    "# Create timestamp column\n",
    "all_times.reset_index(inplace=True)\n",
    "all_times.columns = [timestamp]\n",
    "\n",
    "# Create other time-related columns if you need them in RTS\n",
    "# all_times['month'] = all_times[timestamp].dt.month.astype(str)\n",
    "# all_times['year'] = all_times[timestamp].dt.year.astype(str)\n",
    "# all_times['quarter'] = all_times[timestamp].dt.quarter.astype(str)\n",
    "# all_times['year_month'] = all_times['year'] + '_' + all_times[timestamp].dt.month\n",
    "all_times['day_of_week'] = all_times[timestamp].dt.day_name().astype(str)\n",
    "all_times['hour_of_day'] = all_times[timestamp].dt.hour.astype(str)\n",
    "all_times['day_hour_name'] = all_times['day_of_week'] + \"_\" + all_times['hour_of_day']\n",
    "all_times['weekend_flag'] = all_times[timestamp].dt.dayofweek\n",
    "all_times['weekend_flag'] = (all_times['weekend_flag'] >= 5).astype(int)\n",
    "all_times['is_sun_mon'] = 0\n",
    "all_times.loc[((all_times.day_of_week==\"Sunday\") | (all_times.day_of_week==\"Monday\")), 'is_sun_mon'] = 1\n",
    "\n",
    "print(all_times.dtypes)\n",
    "print(all_times.isna().sum())\n",
    "all_times.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create master template of all possible locations and items\n",
    "items = df.groupby([item_id, location_id]).min()\n",
    "\n",
    "# # subset just top items\n",
    "# items = tts_top.groupby([item_id, location_id]).min()\n",
    "\n",
    "# drop timestamp\n",
    "items.reset_index(inplace=True)\n",
    "items.drop([timestamp, target_value], axis=1, inplace=True)\n",
    "# items.head(2)\n",
    "\n",
    "if use_location:\n",
    "    locations = pd.DataFrame(list(tts[location_id].unique()))\n",
    "    locations.columns = [location_id]\n",
    "    # locations.head()\n",
    "    locations['key'] = 1\n",
    "    items['key'] = 1\n",
    "    # Do the cross-join\n",
    "    master_records = locations.merge(items, on ='key').drop(\"key\", 1) \n",
    "    print(master_records.shape, items.shape, locations.shape)\n",
    "    num_locs = len(master_records[location_id].value_counts())\n",
    "    print(f\"num locations: {num_locs}\")\n",
    "else:\n",
    "    master_records = items.copy()\n",
    "    print(master_records.shape, items.shape)\n",
    "\n",
    "# check you did the right thing\n",
    "num_items = len(master_records[item_id].value_counts())\n",
    "master_records.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-join to create master template of all possible locations and items and times\n",
    "all_times['key'] = 1\n",
    "master_records['key'] = 1\n",
    "\n",
    "# Do the cross-join\n",
    "full_history = all_times.merge(master_records, on ='key').drop(\"key\", 1) \n",
    "\n",
    "# make sure you don't have any nulls\n",
    "print(full_history.shape)\n",
    "print(\"checking nulls...\")\n",
    "print(full_history.isna().sum())\n",
    "full_history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in original target_value\n",
    "\n",
    "temp_target = df[[timestamp, item_id, target_value]].copy()\n",
    "\n",
    "# merge in target_value\n",
    "temp2 = full_history.copy()\n",
    "temp = temp2.merge(temp_target, how=\"left\", on=[timestamp, item_id])\n",
    "print(temp.shape, full_history.shape)\n",
    "\n",
    "# check nulls\n",
    "print(temp.isna().sum())\n",
    "temp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with merged values\n",
    "full_history = temp.copy()\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each time series, calculate ADI\n",
    "temp = full_history.copy()\n",
    "temp = temp.groupby([item_id]).apply(lambda x: calc_ADI(x, timestamp, item_id, target_value))\n",
    "temp.reset_index(drop=True, inplace=True)\n",
    "display(temp.sample(3))\n",
    "\n",
    "# Capture each time series class counts\n",
    "ts_counts = temp.groupby(item_id)[['ts_type']].first()\n",
    "ts_counts.reset_index(inplace=True)\n",
    "ts_counts = ts_counts.ts_type.value_counts(dropna=False)\n",
    "print(\"TIME SERIES CLASSES: \")\n",
    "display(ts_counts)\n",
    "print(f\"NUMBER OF DISTINCT CLASSES OF TIME SERIES = {temp.ts_type.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp.shape, full_history.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with class values\n",
    "full_history = temp.copy()\n",
    "del temp\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series classes using tree map\n",
    "ts_classes = pd.DataFrame(ts_counts)\n",
    "ts_classes.reset_index(inplace=True, drop=False)\n",
    "ts_classes.columns = ['ts_type', 'ts_type_count']\n",
    "sum_total = ts_classes['ts_type_count'].sum()\n",
    "ts_classes['ts_type_percent'] = 100.0 * ts_classes['ts_type_count'] / sum_total\n",
    "ts_classes['ts_type_percent'] = ts_classes['ts_type_percent'].round(2)\n",
    "display(ts_classes)\n",
    "\n",
    "# tree map\n",
    "lbl = [f\"{c[0]} = \\n{c[1]}%\" for c in zip(ts_classes.ts_type, ts_classes.ts_type_percent)]\n",
    "squarify.plot(\n",
    "    label=lbl\n",
    "    , sizes=list(ts_classes.ts_type_count)\n",
    "    , color=colors\n",
    "    , text_kwargs={'fontsize':14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some erratic time series\n",
    "erratic_sample = full_history.loc[(full_history.ts_type==\"erratic\"), [timestamp, item_id, target_value]]\n",
    "erratic_sample.set_index(timestamp, inplace=True)\n",
    "erratic_sample.head(2)\n",
    "\n",
    "# random sample 5\n",
    "erratic_items = erratic_sample[item_id].sample(5)\n",
    "erratic_items = erratic_items.reset_index()\n",
    "erratic_items = erratic_items[item_id].unique()\n",
    "erratic_items\n",
    "\n",
    "# make plots\n",
    "make_plots(erratic_sample, use_location, erratic_items, sample_locations=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some regular time series\n",
    "regular_sample = full_history.loc[(full_history.ts_type==\"regular\"), [timestamp, item_id, target_value]]\n",
    "regular_sample.set_index(timestamp, inplace=True)\n",
    "regular_sample.head(2)\n",
    "\n",
    "# random sample 5\n",
    "regular_items = regular_sample[item_id].sample(5)\n",
    "regular_items = regular_items.reset_index()\n",
    "regular_items = regular_items[item_id].unique()\n",
    "regular_items\n",
    "\n",
    "# make plots\n",
    "make_plots(regular_sample, use_location, regular_items, sample_locations=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some intermittent time series\n",
    "intermittent_sample = full_history.loc[(full_history.ts_type==\"intermittent\"), [timestamp, item_id, target_value]]\n",
    "intermittent_sample.set_index(timestamp, inplace=True)\n",
    "intermittent_sample.head(2)\n",
    "\n",
    "# random sample 5\n",
    "intermittent_items = intermittent_sample[item_id].sample(5)\n",
    "intermittent_items = intermittent_items.reset_index()\n",
    "intermittent_items = intermittent_items[item_id].unique()\n",
    "intermittent_items\n",
    "\n",
    "# make plots\n",
    "make_plots(intermittent_sample, use_location, intermittent_items, sample_locations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some lumpy time series\n",
    "lumpy_sample = full_history.loc[(full_history.ts_type==\"lumpy\"), [timestamp, item_id, target_value]]\n",
    "lumpy_sample.set_index(timestamp, inplace=True)\n",
    "lumpy_sample.head(2)\n",
    "\n",
    "# random sample 5\n",
    "lumpy_items = lumpy_sample[item_id].sample(5)\n",
    "lumpy_items = lumpy_items.reset_index()\n",
    "lumpy_items = lumpy_items[item_id].unique()\n",
    "lumpy_items\n",
    "\n",
    "# make plots\n",
    "make_plots(lumpy_sample, use_location, lumpy_items, sample_locations=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only \"erratic\" time series are worth predicting.  All the rest look either suspiciously fake or too few data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Split train/test data  <a class=\"anchor\" id=\"split_train_test\"></a> \n",
    "\n",
    "In forecasting, \"train\" data is until a last-train date, sometimes called the forecast snapshot date.  \n",
    "<ul>\n",
    "    <li>Train data includes all data up to your last-train date. </li>\n",
    "    <li>Test data includes dates after your last-train date through end of desired forecast horizon.</li>\n",
    "    <li>Validation data might exist for part or maybe all of the desired forecast horizon. </li>\n",
    "    <li>TTS timestamps should start and end with Train data. </li>\n",
    "    <li>RTS timestamps should start with Train data and extend out past end of TTS to end of the desired forecast horizon.</li>\n",
    "    </ul>\n",
    "    \n",
    "For model generalization, all processing from here on out will only be done on train data.\n",
    "\n",
    "\n",
    "TODO:  Adjust this section so you can 1) create RTS with known data, or 2) create RTS with unknown future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Horizon is number of time steps out in the future you want to predict\n",
    "# Time steps are defined in the time frequency you specified in Step 5 Aggregate\n",
    "\n",
    "# Example if aggregation was hourly, then forecast length=168 means forecast horizon of 7 days or 7*24=168 hours\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH}\") # = 12\n",
    "print(f\"Forecast unit of frequency = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\") # = 30\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train data as all except last FORECAST_HORIZON length\n",
    "start_time = df[timestamp].min()\n",
    "end_time = snapshot_end\n",
    "start_time_test = SNAPSHOT_DATE\n",
    "\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time_train = {end_time_train}\")\n",
    "print(f\"start_time_test = {start_time_test}\")\n",
    "print(f\"end_time = {snapshot_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_future_RTS_with_unknown_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_future_RTS_with_unknown_data:\n",
    "    # Create train data as all data => this means RTS will extend into unknown future\n",
    "    print(\"using all known data for training\")\n",
    "    train_df = df_clean.copy()\n",
    "else:\n",
    "    # Create train subset with hold-out of length FORECAST_LENGTH\n",
    "    print(\"using hold-out with train data\")\n",
    "    train_df = df_clean.copy()\n",
    "    train_df = train_df.loc[(train_df[timestamp] <= end_time_train), :]\n",
    "\n",
    "# check you did the right thing\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time: {end_time}\")\n",
    "print()\n",
    "print(f\"start_time_train = {train_df[timestamp].min()}\")\n",
    "print(f\"end_time_train = {train_df[timestamp].max()}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CHECK: DO YOU HAVE ENOUGH HISTORICAL DATA POINTS TO SUPPORT DESIRED FORECAST HORIZON?\n",
    "\n",
    "# calculate number data points in train data\n",
    "num_data_points = train_df.groupby([item_id]).nunique()[timestamp].mean()\n",
    "print(f\"1/3 training data points: {np.round(num_data_points/3,0)}\")\n",
    "\n",
    "# Amazon Forecast length of forecasts can be 500 data points and 1/3 target time series dataset length\n",
    "if ((FORECAST_LENGTH < 500) & (FORECAST_LENGTH <= np.round(num_data_points/3,0))):\n",
    "    print(f\"Success, forecast horizon {FORECAST_LENGTH} is shorter than 500 data points and 1/3 train data\")\n",
    "else:\n",
    "    print(f\"Error, forecast horizon {FORECAST_LENGTH} is too long.  Need fewer than 500 data points and 1/3 train data. \")\n",
    "    \n",
    "# If you have too few data points, return to step above and choose smaller time granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15. Prepare and Save Target Time Series (TTS) <a class=\"anchor\" id=\"TTS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_future_RTS_with_unknown_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble TTS required columns\n",
    "\n",
    "tts = train_df[[timestamp, item_id, location_id, target_value]].copy()\n",
    "tts = tts.groupby([timestamp, item_id, location_id]).sum()\n",
    "tts.reset_index(inplace=True)\n",
    "\n",
    "# check it\n",
    "print(tts.shape)\n",
    "print(f\"start date = {tts[timestamp].min()}\")\n",
    "print(f\"end date = {tts[timestamp].max()}\")\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check format of geolocation column\n",
    "# tts.pickup_geolocation.value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is the last step for TTS. </b>\n",
    "\n",
    "Note: Currently in Amazon Forecast, if you declare target_value is integer in the schema, but you have any decimals in your numbers, you will get an error.\n",
    "\n",
    "Make sure you really see integers in the code below, if you want integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the new pandas Integer type\n",
    "# # https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "# # TODO: turn this into a function\n",
    "\n",
    "# if target_to_integer:\n",
    "#     try:\n",
    "#         tts[target_value] = tts[target_value].astype(int)\n",
    "#         print(\"Success! Converted to numpy integer\")\n",
    "#     except Exception as e:\n",
    "#         print (e)\n",
    "#         print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "#         try:\n",
    "#             tts[target_value] = tts[target_value].fillna(0).astype('Int64', errors='ignore')\n",
    "#             print(\"Success! converted to pandas integer\")\n",
    "#         except Exception as e:\n",
    "#             print (e)\n",
    "# elif tts[target_value].dtype == 'object':\n",
    "#     # convert to float\n",
    "#     tts[target_value] = tts[target_value].astype(np.float32)\n",
    "# elif tts[target_value].dtype != 'object':\n",
    "#     # do nothing\n",
    "#     print(\"target_value is already a float\")\n",
    "    \n",
    "# print(tts.dtypes)\n",
    "# tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save tts to S3\n",
    "# local_file = \"tts.csv\"\n",
    "# # Save merged file locally\n",
    "# tts.to_csv(local_file, header=True, index=False)\n",
    "\n",
    "# # Save file to S3\n",
    "# key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.csv\"\n",
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16. Remove time series with no target values at all<a class=\"anchor\" id=\"TTS-remove-all0\"></a>\n",
    "\n",
    "In case there are time series which are only 0's, may as well remove them, since their forecast should be all 0's too.  Another reason to remove these time series is they could bias the overall forecast toward 0, when that's not what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if sum of all sales is 0\n",
    "g = tts.groupby([item_id, location_id]).sum()\n",
    "skus_with_no_sales_in_warehouse = g[g[target_value] == 0].copy()\n",
    "\n",
    "skus_with_no_sales_in_warehouse.reset_index(inplace=True)\n",
    "skus_with_no_sales_in_warehouse.drop(target_value, inplace=True, axis=1)\n",
    "# print(skus_with_too_few_sales.shape)\n",
    "# display (skus_with_no_sales_in_warehouse.head(2))\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "        \n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_no_sales_in_warehouse, how='left', on=[item_id, location_id], indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with only 0's.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with only 0's\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_no_sales_in_warehouse = skus_with_no_sales_in_warehouse[[item_id, location_id]].copy()\n",
    "    skus_with_no_sales_in_warehouse['reason'] = \"All 0's\"\n",
    "    display(skus_with_no_sales_in_warehouse.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17. Remove time series with end of life<a class=\"anchor\" id=\"TTS-remove-all_end_of_life\"></a>\n",
    "\n",
    "Check if time series have any data in last 6 months and more than 5 data points, since 5 data points is minimum for Amazon Forecast to generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define end of life = No sales in the last 6 months\n",
    "\n",
    "# first get df of only last 6 months\n",
    "time_threshold = end_time - datetime.timedelta(6*30) \n",
    "\n",
    "# check if sum of sales last 6 months is 0\n",
    "tts_aux = tts[tts[timestamp] >= time_threshold].copy()\n",
    "g = tts_aux.groupby([item_id, location_id]).sum()\n",
    "\n",
    "skus_with_end_of_life = g[g[target_value] == 0].copy()\n",
    "skus_with_end_of_life.reset_index(inplace=True)\n",
    "# dropping the target_value, because otherwise it will only drop the '0's and not the other values\n",
    "skus_with_end_of_life.drop(target_value, axis=1, inplace=True)\n",
    "# print(skus_with_end_of_life.shape)\n",
    "# display (skus_with_end_of_life.head(2))\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_end_of_life, how='left', on=[item_id, location_id], indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with end of life.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop the skus with end of life\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    display(tts.dtypes)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason\n",
    "    skus_with_end_of_life = skus_with_end_of_life[[item_id, location_id]].copy()\n",
    "    skus_with_end_of_life['reason'] = \"end of life\"\n",
    "    display(skus_with_end_of_life.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18. Remove time series with fewer than 5 data points<a class=\"anchor\" id=\"TTS_remove_too_few_data_points\"></a>\n",
    "\n",
    "Note:  Minimum number of data points is 5 data points to make a forecast.  You can run this to remove them manually and save the list of time series with too few data points for your own reference.  Otherwise if you skip this section, Forecast will automatically drop (silently) all time series with fewer than 5 data points, since that is too few to make a good forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Replacing '0's with null</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Null-value filling, if any\n",
    "\n",
    "# special case:  replace 0s with nulls\n",
    "if (replace_all_zeroes_with_null):\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "    if target_to_integer:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    else:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = np.nan\n",
    "    print ()\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "else:\n",
    "    tts.loc[:, target_value].fillna(0, inplace=True)\n",
    "    print(\"No null-filling required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check per time series if count of data points is at least 5\n",
    "g = tts.groupby([item_id, location_id]).count()\n",
    "skus_with_too_few_sales = g[g[target_value] < 5].copy()\n",
    "\n",
    "skus_with_too_few_sales.reset_index(inplace=True)\n",
    "skus_with_too_few_sales.drop([target_value, timestamp], inplace=True, axis=1)\n",
    "skus_with_too_few_sales.drop_duplicates(inplace=True)\n",
    "print(\"items with too few data points\")\n",
    "print(skus_with_too_few_sales.shape)\n",
    "display (skus_with_too_few_sales.head(2))\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_too_few_sales, how='left', on=[item_id, location_id], indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(\"TTS if you dropped items with too few data points\")\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with fewer than 5 datapoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with too few data points, only if more than a handful found\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_too_few_sales = skus_with_too_few_sales[[item_id, location_id]].copy()\n",
    "    skus_with_too_few_sales['reason'] = \"Fewer than 5 datapoints\"\n",
    "    display(skus_with_too_few_sales.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Keep track of dropped time series and reason why they were dropped. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped_dims = skus_with_too_few_sales.append([skus_with_no_sales_in_warehouse\n",
    "                                                       , skus_with_end_of_life])\n",
    "    print(f\"unique ts dropped = {dropped_dims.shape[0]}\")\n",
    "    print(f\"unique ts fewer than 5 data points = {skus_with_too_few_sales.shape[0]}\")\n",
    "    print(f\"unique ts with all 0s = {skus_with_no_sales_in_warehouse.shape[0]}\")\n",
    "    print(f\"unique ts with end of life = {skus_with_end_of_life.shape[0]}\")\n",
    "    display(dropped_dims.reason.value_counts(dropna=False, normalize=True))\n",
    "    display(dropped_dims.sample(1))\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of dropped skus and reasons for reference and to check if data can be fixed\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 2:\n",
    "    # save all the dropped dimensions fields\n",
    "    local_file = \"dropped_fields.csv\"\n",
    "    # Save merged file locally\n",
    "    dropped_dims.to_csv(local_file, header=True, index=False)\n",
    "\n",
    "    key = f\"{prefix}/v{DATA_VERSION}/dropped_{EXPERIMENT_NAME}.csv\"\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is last step for TTS. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "display(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new pandas Integer type\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "\n",
    "if target_to_integer:\n",
    "    try:\n",
    "        tts[target_value] = tts[target_value].fillna(0).astype(int)\n",
    "        print(\"Success! Converted to np.integer type\")\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "        try:\n",
    "            tts[target_value] = tts[target_value].fillna(0).astype('Int64', errors='ignore')\n",
    "            print(\"Success! Converted to nullable pd.integer type\")\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "elif tts[target_value].dtype == 'object':\n",
    "    # convert to float\n",
    "    tts[target_value] = tts[target_value].astype(np.float32)\n",
    "elif tts[target_value].dtype != 'object':\n",
    "    # do nothing\n",
    "    print(\"target_value is already a float\")\n",
    "    \n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Optional - replace 0's with nulls </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_all_zeroes_with_null:\n",
    "    tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    \n",
    "print(tts[target_value].describe())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last check...\n",
    "print(tts.shape)\n",
    "tts.drop_duplicates(inplace=True)\n",
    "print(tts.shape)\n",
    "print(tts[timestamp].min())\n",
    "print(tts[timestamp].max())\n",
    "# check for nulls\n",
    "print(tts.isnull().sum())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "tts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input numbers of time series\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped = dropped_dims.groupby([item_id, location_id]).max().shape[0]\n",
    "    display(dropped)\n",
    "    # check\n",
    "    assert (train_df[item_id].nunique() == (tts[item_id].nunique() + dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkit = pd.read_csv(\"s3://christy-forecast/open-data-analytics-taxi-trips/v6/experiment3_top.csv\")\n",
    "# print(checkit.shape)\n",
    "# checkit.columns = [timestamp, item_id, location_id, target_value]\n",
    "# checkit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(checkit[timestamp].min())\n",
    "# print(checkit[timestamp].max())\n",
    "# print(checkit.isnull().sum())\n",
    "# checkit[target_value].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19. Optional - Assemble and save TTS_sparse, TTS_dense <a class=\"anchor\" id=\"TTS-dense_sparse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num data points per item\n",
    "sparsity = tts.copy()\n",
    "sparsity = sparsity.groupby([item_id, location_id]).count()\n",
    "sparsity[[target_value]].hist(bins=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sparsity[target_value].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# list the \"dense\" item combinations\n",
    "sparsity.reset_index(inplace=True)\n",
    "dense = sparsity.loc[(sparsity[target_value] >= stats['75%']), [item_id, location_id]].drop_duplicates()\n",
    "print(dense.shape, sparsity.drop_duplicates().shape)\n",
    "dense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# all the rest are sparse\n",
    "sparse = sparsity.merge(dense, how='outer', indicator=True) \\\n",
    "           .query(\"_merge=='left_only'\") \\\n",
    "           .drop('_merge',1)[[item_id, location_id]]\n",
    "print(sparse.shape, dense.shape, sparsity.drop_duplicates().shape)\n",
    "sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at some singletons\n",
    "sparsity.loc[(sparsity[target_value] <5), [item_id, location_id]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spot-check a singleton\n",
    "# df.loc[((df[item_id]==\"204\")), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sparse dimensions\n",
    "local_file = \"sparse_fields.csv\"\n",
    "# Save merged file locally\n",
    "sparse.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/sparse_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dense dimensions\n",
    "local_file = \"dense_fields.csv\"\n",
    "# Save merged file locally\n",
    "dense.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/dense_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just dense time series\n",
    "tts_dense = tts.merge(dense, how=\"inner\", on=[item_id, location_id])\n",
    "print(tts_dense.shape, tts.shape)\n",
    "tts_dense.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20. Optional - Assemble and save tts.top, tts.slow <a class=\"anchor\" id=\"TTS_top\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# assemble tts_top\n",
    "if use_location:\n",
    "    tts_top = tts.loc[(tts[item_id].isin(list(top_moving_ts[item_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_top = tts.loc[(tts[item_id].isin(list(top_moving_items[item_id].unique()))), :].copy()\n",
    "\n",
    "print(tts_top.shape, tts.shape)\n",
    "num_top_items = tts_top[item_id].nunique()\n",
    "print(f\"Number top items = {num_top_items}\")\n",
    "tts_top.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_top to S3\n",
    "local_file = \"tts_top.csv\"\n",
    "# Save merged file locally\n",
    "tts_top.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_top_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# assemble tts_slow\n",
    "if use_location:\n",
    "    tts_slow = tts.loc[(tts[item_id].isin(list(slow_moving_ts[item_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_slow = tts.loc[(tts[item_id].isin(list(slow_moving_items[item_id].unique()))), :].copy()\n",
    "\n",
    "\n",
    "print(tts_slow.shape, tts.shape)\n",
    "num_slow_items = tts_slow[item_id].nunique()\n",
    "print(f\"Number slow items = {num_slow_items}\")\n",
    "tts_slow.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_slow to S3\n",
    "local_file = \"tts_slow.csv\"\n",
    "# Save merged file locally\n",
    "tts_slow.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_slow_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert num_slow_items + num_top_items == tts[item_id].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21. Optional - Assemble and save TTS_regular, TTS_erratic, TTS_intermittent, TTS_lumpy <a class=\"anchor\" id=\"TTS_classes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# restrict tts to just regular time series\n",
    "tts_regular = tts.loc[(tts[item_id].isin(list(regular_sample[item_id].unique()))), :].copy()\n",
    "# tts_reg = tts.merge(regular, how=\"inner\", on=[item_id, location_id])\n",
    "\n",
    "print(tts_regular.shape, tts.shape)\n",
    "num_regular_items = tts_regular[item_id].nunique()\n",
    "print(f\"Number regular items = {num_regular_items}\")\n",
    "tts_regular.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# restrict tts to just erratic time series\n",
    "tts_erratic = tts.loc[(tts[item_id].isin(list(erratic_sample[item_id].unique()))), :].copy()\n",
    "# tts_reg = tts.merge(erratic, how=\"inner\", on=[item_id, location_id])\n",
    "\n",
    "print(tts_erratic.shape, tts.shape)\n",
    "num_erratic_items = tts_erratic[item_id].nunique()\n",
    "print(f\"Number erratic items = {num_erratic_items}\")\n",
    "tts_erratic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# restrict tts to just intermittent time series\n",
    "tts_intermittent = tts.loc[(tts[item_id].isin(list(intermittent_sample[item_id].unique()))), :].copy()\n",
    "# tts_reg = tts.merge(intermittent, how=\"inner\", on=[item_id, location_id])\n",
    "\n",
    "print(tts_intermittent.shape, tts.shape)\n",
    "num_intermittent_items = tts_intermittent[item_id].nunique()\n",
    "print(f\"Number intermittent items = {num_intermittent_items}\")\n",
    "tts_intermittent.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - drop everything in dropped_dims\n",
    "try:\n",
    "    display(dropped_dims)\n",
    "except:\n",
    "    print(\"Didn't drop anything.\")\n",
    "\n",
    "# restrict tts to just lumpy time series\n",
    "tts_lumpy = tts.loc[(tts[item_id].isin(list(lumpy_sample[item_id].unique()))), :].copy()\n",
    "# tts_reg = tts.merge(lumpy, how=\"inner\", on=[item_id, location_id])\n",
    "\n",
    "print(tts_lumpy.shape, tts.shape)\n",
    "num_lumpy_items = tts_lumpy[item_id].nunique()\n",
    "print(f\"Number lumpy items = {num_lumpy_items}\")\n",
    "tts_lumpy.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_regular to S3\n",
    "local_file = \"tts_regular.csv\"\n",
    "# Save merged file locally\n",
    "tts_regular.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_regular_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_erratic to S3\n",
    "local_file = \"tts_erratic.csv\"\n",
    "# Save merged file locally\n",
    "tts_erratic.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_erratic_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_intermittent to S3\n",
    "local_file = \"tts_intermittent.csv\"\n",
    "# Save merged file locally\n",
    "tts_intermittent.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_intermittent_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_lumpy to S3\n",
    "local_file = \"tts_lumpy.csv\"\n",
    "# Save merged file locally\n",
    "tts_lumpy.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_lumpy_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert num_regular_items + num_erratic_items + num_intermittent_items + num_lumpy_items == tts[item_id].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22. Assemble and save metadata (if any) <a class=\"anchor\" id=\"IM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metadata columns\n",
    "im = df_clean[[item_id, 'pickup_borough', 'binned_max_item']].copy()\n",
    "im = im.groupby(item_id).first()\n",
    "im.reset_index(inplace=True)\n",
    "# check nulls\n",
    "display(im.isnull().sum())\n",
    "\n",
    "print(im.shape)\n",
    "if im.shape[0] < 50:\n",
    "    print(im)\n",
    "else:\n",
    "    print(im.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cardinality of metadata columns\n",
    "im.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional metadata using just item target_value that is sometimes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate sales by item \n",
    "# synthetic = df.copy()\n",
    "# synthetic = (synthetic.groupby([item_id])\n",
    "#         .agg({target_value: ['max']}))\n",
    "\n",
    "# synthetic = synthetic.reset_index()\n",
    "# synthetic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #bin data into 4 categories\n",
    "# cat_scales = [\"Cat_{}\".format(i) for i in range(1,5)]\n",
    "# synthetic['item_cat_by_max'] = list(pd.cut(synthetic[target_value]['max'].values, 4, labels=cat_scales))\n",
    "\n",
    "# synthetic.drop(target_value, axis=1, inplace=True)\n",
    "# synthetic.columns = synthetic.columns.get_level_values(0)\n",
    "\n",
    "# print(synthetic.shape)\n",
    "# print(synthetic.dtypes)\n",
    "# print(synthetic.columns)\n",
    "# synthetic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic.item_cat_by_max.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge synthetic features\n",
    "# im = im.merge(synthetic, how=\"left\", on=[item_id])\n",
    "# print(im.shape, synthetic.shape)\n",
    "# im.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble metadata\n",
    "\n",
    "im = im.groupby(item_id).max()\n",
    "im.reset_index(inplace=True)\n",
    "print(im.shape)\n",
    "print(\"checking nulls..\")\n",
    "print(im.isnull().sum())\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save im to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "im.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.metadata.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23. Prepare and save RTS (if any) <a class=\"anchor\" id=\"RTS\"></a>\n",
    "\n",
    "Tip:  Make sure RTS does not have any missing values, even if RTS extends into future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate mean sales per item per year\n",
    "\n",
    "# temp_year_item = train_df[['year', item_id, target_value]].copy()\n",
    "# temp_year_item.year = temp_year_item.year.astype(str)\n",
    "# temp_year_item = temp_year_item.groupby(['year', item_id]).mean()\n",
    "# temp_year_item.reset_index(inplace=True)\n",
    "# temp_year_item.rename(columns={target_value:\"count_year_item\"}, inplace=True)\n",
    "# print(temp_year_item.dtypes)\n",
    "# temp_year_item.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge in year-item trend\n",
    "\n",
    "# temp2 = full_history.copy()\n",
    "# # temp.drop(\"count_day_loc_item\", inplace=True, axis=1)\n",
    "# print(temp2.shape)\n",
    "# temp = temp2.merge(temp_year_item, how=\"left\", on=[\"year\", item_id])\n",
    "# print(temp.shape, temp_year_item.shape)\n",
    "\n",
    "# # check nulls\n",
    "# print(temp.isna().sum())\n",
    "# temp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Careful!!\n",
    "# # Really replace full_history with merged values\n",
    "# full_history = temp.copy()\n",
    "# full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge in sparse or not column\n",
    "# sparse['is_sparse'] = 1\n",
    "# full_history['is_sparse'] = 0\n",
    "\n",
    "# print(full_history.is_sparse.describe())\n",
    "# full_history = full_history.merge(sparse, how=\"left\", on=[item_id, location_id])\n",
    "# full_history['is_sparse'] = full_history.is_sparse_y.combine_first(full_history.is_sparse_x)\n",
    "# full_history.drop(['is_sparse_x', 'is_sparse_y'], inplace=True, axis=1)\n",
    "# print(full_history.is_sparse.describe())\n",
    "# full_history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in top_moving or not column\n",
    "# TODO: double-check this for case when locations used\n",
    "\n",
    "full_history['top_moving'] = 0\n",
    "if use_location:\n",
    "    full_history.loc[(full_history[item_id].isin(top_moving_ts[item_id])\n",
    "                     & (full_history[location_id].isin(top_moving_ts[location_id]))), 'top_moving'] = 1\n",
    "else:\n",
    "    full_history.loc[(full_history[item_id].isin(tts_top[item_id])), 'top_moving'] = 1\n",
    "print(full_history.top_moving.value_counts(normalize=True, dropna=False))\n",
    "print()\n",
    "full_history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in time slice so you can see patterns\n",
    "df_plot = full_history.loc[((full_history[timestamp]>\"2020-01-10\")\n",
    "                           & (full_history[timestamp]<end_time_train)\n",
    "                           & (full_history[item_id].isin(random_items))), :].copy()\n",
    "print(df_plot.shape, full_history.shape)\n",
    "df_plot = df_plot.groupby([timestamp]).sum()\n",
    "df_plot.reset_index(inplace=True)\n",
    "df_plot.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[target_value].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize candidate RTS variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='weekend_flag', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom-in.  Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.loc[(df_plot[timestamp]>\"2020-02-15\"), :].plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.loc[(df_plot[timestamp]>\"2020-02-15\"), :].plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lowest taxis rides are wee hours of Sunday morning through Mondays.  So it's a combination of day and hour that seems to matter, not just day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble RTS - include whatever columns you finally decide\n",
    "\n",
    "rts = full_history[[timestamp, item_id, location_id, 'day_hour_name']].copy()\n",
    "\n",
    "print(rts.shape)\n",
    "print(rts.isnull().sum())\n",
    "print(f\"rts start: {rts[timestamp].min()}\")\n",
    "print(f\"rts end: {rts[timestamp].max()}\")\n",
    "rts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rts to S3\n",
    "local_file = \"rts.csv\"\n",
    "# Save merged file locally\n",
    "rts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.related.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkit = pd.read_csv(\"s3://christy-forecast/open-data-analytics-taxi-trips/v6/experiment3.related.csv\")\n",
    "# print(checkit.shape)\n",
    "# checkit.columns = [timestamp, item_id, location_id, \"day_hour_name\"]\n",
    "# checkit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(checkit[timestamp].min())\n",
    "# print(checkit[timestamp].max())\n",
    "# print(checkit.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
